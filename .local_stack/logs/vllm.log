/home/zodrak/aithrt/bin/python: Error while finding module specification for 'vllm.entrypoints.openai.api_server' (ModuleNotFoundError: No module named 'vllm')
/home/zodrak/aithrt/lib/python3.13/site-packages/torch/cuda/__init__.py:827: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/home/zodrak/aithrt/lib/python3.13/site-packages/torch/cuda/__init__.py:1034: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 304: OS call failed or operation not supported on this OS (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:119.)
  r = torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count
INFO 02-04 09:49:09 [importing.py:44] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.
INFO 02-04 09:49:09 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
/home/zodrak/aithrt/lib/python3.13/site-packages/torch/cuda/__init__.py:827: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
W0204 09:49:10.139000 743745 torch/utils/cpp_extension.py:117] No CUDA runtime is found, using CUDA_HOME='/usr'
Traceback (most recent call last):
  File "/home/zodrak/aithrt/bin/vllm", line 8, in <module>
    sys.exit(main())
             ~~~~^^
  File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/entrypoints/cli/main.py", line 66, in main
    cmd.subparser_init(subparsers).set_defaults(dispatch_function=cmd.cmd)
    ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/entrypoints/cli/serve.py", line 127, in subparser_init
    serve_parser = make_arg_parser(serve_parser)
  File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/entrypoints/openai/cli_args.py", line 300, in make_arg_parser
    parser = AsyncEngineArgs.add_cli_args(parser)
  File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/engine/arg_utils.py", line 2040, in add_cli_args
    parser = EngineArgs.add_cli_args(parser)
  File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/engine/arg_utils.py", line 1153, in add_cli_args
    vllm_kwargs = get_kwargs(VllmConfig)
  File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/engine/arg_utils.py", line 346, in get_kwargs
    return copy.deepcopy(_compute_kwargs(cls))
                         ~~~~~~~~~~~~~~~^^^^^
  File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/engine/arg_utils.py", line 258, in _compute_kwargs
    default = default.default_factory()
  File "/home/zodrak/aithrt/lib/python3.13/site-packages/pydantic/_internal/_dataclasses.py", line 121, in __init__
    s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/config/device.py", line 58, in __post_init__
    raise RuntimeError(
    ...<3 lines>...
    )
RuntimeError: Failed to infer device type, please set the environment variable `VLLM_LOGGING_LEVEL=DEBUG` to turn on verbose logging to help debug the issue.
[0;36m(APIServer pid=747763)[0;0m INFO 02-04 11:35:23 [utils.py:325] 
[0;36m(APIServer pid=747763)[0;0m INFO 02-04 11:35:23 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=747763)[0;0m INFO 02-04 11:35:23 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
[0;36m(APIServer pid=747763)[0;0m INFO 02-04 11:35:23 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   Qwen/Qwen2.5-3B-Instruct-AWQ
[0;36m(APIServer pid=747763)[0;0m INFO 02-04 11:35:23 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=747763)[0;0m INFO 02-04 11:35:23 [utils.py:325] 
[0;36m(APIServer pid=747763)[0;0m INFO 02-04 11:35:23 [utils.py:261] non-default args: {'model_tag': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'api_server_count': 1, 'host': '0.0.0.0', 'model': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'max_model_len': 8192, 'quantization': 'awq', 'gpu_memory_utilization': 0.5}
[0;36m(APIServer pid=747763)[0;0m INFO 02-04 11:35:28 [model.py:541] Resolved architecture: Qwen2ForCausalLM
[0;36m(APIServer pid=747763)[0;0m INFO 02-04 11:35:28 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=747763)[0;0m INFO 02-04 11:35:28 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference
[0;36m(APIServer pid=747763)[0;0m INFO 02-04 11:35:29 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=747763)[0;0m INFO 02-04 11:35:29 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=747894)[0;0m INFO 02-04 11:35:37 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='Qwen/Qwen2.5-3B-Instruct-AWQ', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen2.5-3B-Instruct-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=747894)[0;0m INFO 02-04 11:35:38 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.0.76:42089 backend=nccl
[0;36m(EngineCore_DP0 pid=747894)[0;0m INFO 02-04 11:35:38 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=747894)[0;0m INFO 02-04 11:35:38 [gpu_model_runner.py:4021] Starting to load model Qwen/Qwen2.5-3B-Instruct-AWQ...
[0;36m(EngineCore_DP0 pid=747894)[0;0m /home/zodrak/aithrt/lib/python3.13/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[0;36m(EngineCore_DP0 pid=747894)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[0;36m(EngineCore_DP0 pid=747894)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=747894)[0;0m INFO 02-04 11:35:41 [cuda.py:364] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=747894)[0;0m INFO 02-04 11:55:24 [weight_utils.py:527] Time spent downloading weights for Qwen/Qwen2.5-3B-Instruct-AWQ: 1182.537079 seconds
[0;36m(EngineCore_DP0 pid=747894)[0;0m INFO 02-04 11:55:24 [weight_utils.py:567] No model.safetensors.index.json found in remote.
[0;36m(EngineCore_DP0 pid=747894)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=747894)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.82it/s]
[0;36m(EngineCore_DP0 pid=747894)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.82it/s]
[0;36m(EngineCore_DP0 pid=747894)[0;0m 
[0;36m(EngineCore_DP0 pid=747894)[0;0m INFO 02-04 11:55:24 [default_loader.py:291] Loading weights took 0.26 seconds
[0;36m(EngineCore_DP0 pid=747894)[0;0m INFO 02-04 11:55:25 [gpu_model_runner.py:4118] Model loading took 1.95 GiB memory and 1186.125949 seconds
[0;36m(EngineCore_DP0 pid=747894)[0;0m INFO 02-04 11:55:30 [backends.py:805] Using cache directory: /home/zodrak/.cache/vllm/torch_compile_cache/0ed0e99edc/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=747894)[0;0m INFO 02-04 11:55:30 [backends.py:865] Dynamo bytecode transform time: 5.36 s
/tmp/tmp3x_ljhrt/cuda_utils.c:6:10: fatal error: Python.h: No such file or directory
    6 | #include <Python.h>
      |          ^~~~~~~~~~
compilation terminated.
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]         vllm_config,
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     )
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     ^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]         vllm_config
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     )
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     ^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 242, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]            ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/worker/gpu_worker.py", line 322, in determine_available_memory
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4974, in profile_run
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]                                         ~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]         self.max_num_tokens, is_profile=True
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     )
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     ^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4685, in _dummy_run
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]         input_ids=input_ids,
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]         **model_kwargs,
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     )
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/compilation/cuda_graph.py", line 222, in __call__
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]            ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]         input_ids, positions, intermediate_tensors, inputs_embeds
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     )
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/compilation/decorators.py", line 561, in __call__
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)  # type: ignore[arg-type]
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/compilation/wrapper.py", line 228, in __call__
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]         self._compiled_callable, *args, **kwargs
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     )
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     ^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/compilation/wrapper.py", line 119, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]         e.__traceback__
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     ) from None
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]         gm, example_inputs, inputs_to_check, **graph_kwargs
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     )
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]            ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]            ~~~~~~~~~~~~~~~~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]                                                              ~~~~~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/graph.py", line 2260, in codegen
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     self._update_scheduler()
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     ~~~~~~~~~~~~~~~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/graph.py", line 2254, in _update_scheduler
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     self.scheduler = Scheduler(self.operations)
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]                      ~~~~~~~~~^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/scheduler.py", line 2226, in __init__
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     self._init(nodes)
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     ~~~~~~~~~~^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/scheduler.py", line 2320, in _init
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     self.create_combo_kernel_nodes(num_ck_nodes=None)
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/scheduler.py", line 3621, in create_combo_kernel_nodes
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     if not self.speedup_by_combo_kernel(node_list):
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/scheduler.py", line 5509, in speedup_by_combo_kernel
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     ms, path = self.benchmark_fused_nodes(node_list)
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]                ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/scheduler.py", line 3023, in benchmark_fused_nodes
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     return backend.benchmark_fused_nodes(nodes)
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/codegen/cuda_combined_scheduling.py", line 141, in benchmark_fused_nodes
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     return self._triton_scheduling.benchmark_fused_nodes(nodes)
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/codegen/triton.py", line 4778, in benchmark_fused_nodes
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     src_code = self.generate_kernel_code_from_nodes(nodes, benchmark_kernel=True)
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/codegen/simd.py", line 2576, in generate_kernel_code_from_nodes
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     src_code = kernel.codegen_kernel()
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/codegen/triton.py", line 4173, in codegen_kernel
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     **self.inductor_meta_common(),
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]       ~~~~~~~~~~~~~~~~~~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/codegen/triton.py", line 3992, in inductor_meta_common
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     "backend_hash": torch.utils._triton.triton_hash_with_backend(),
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/utils/_triton.py", line 175, in triton_hash_with_backend
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     backend = triton_backend()
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/utils/_triton.py", line 167, in triton_backend
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     target = driver.active.get_current_target()
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]              ^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/triton/runtime/driver.py", line 28, in active
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     self._active = self.default
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]                    ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/triton/runtime/driver.py", line 22, in default
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     self._default = _create_driver()
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]                     ~~~~~~~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/triton/runtime/driver.py", line 10, in _create_driver
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     return active_drivers[0]()
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]            ~~~~~~~~~~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/triton/backends/nvidia/driver.py", line 719, in __init__
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     self.utils = CudaUtils()  # TODO: make static
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]                  ~~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/triton/backends/nvidia/driver.py", line 63, in __init__
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     mod = compile_module_from_src(
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]         src=Path(os.path.join(dirname, "driver.c")).read_text(),
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]         libraries=libraries,
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     )
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/triton/runtime/build.py", line 89, in compile_module_from_src
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     so = _build(name, src_path, tmpdir, library_dirs or [], include_dirs or [], libraries or [], ccflags or [])
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/home/zodrak/aithrt/lib/python3.13/site-packages/triton/runtime/build.py", line 51, in _build
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     subprocess.check_call(cc_cmd, stdout=subprocess.DEVNULL)
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]   File "/usr/lib/python3.13/subprocess.py", line 419, in check_call
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946]     raise CalledProcessError(retcode, cmd)
[0;36m(EngineCore_DP0 pid=747894)[0;0m ERROR 02-04 11:55:34 [core.py:946] torch._inductor.exc.InductorError: CalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmp3x_ljhrt/cuda_utils.c', '-O3', '-shared', '-fPIC', '-Wno-psabi', '-o', '/tmp/tmp3x_ljhrt/cuda_utils.cpython-313-x86_64-linux-gnu.so', '-lcuda', '-L/home/zodrak/aithrt/lib/python3.13/site-packages/triton/backends/nvidia/lib', '-L/lib/x86_64-linux-gnu', '-I/home/zodrak/aithrt/lib/python3.13/site-packages/triton/backends/nvidia/include', '-I/tmp/tmp3x_ljhrt', '-I/usr/include/python3.13']' returned non-zero exit status 1.
[0;36m(EngineCore_DP0 pid=747894)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=747894)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/usr/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
[0;36m(EngineCore_DP0 pid=747894)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=747894)[0;0m     ~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/usr/lib/python3.13/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=747894)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=747894)[0;0m     ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=747894)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=747894)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=747894)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=747894)[0;0m     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=747894)[0;0m         vllm_config,
[0;36m(EngineCore_DP0 pid=747894)[0;0m         ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=747894)[0;0m         internal_dp_balancing,
[0;36m(EngineCore_DP0 pid=747894)[0;0m         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m     )
[0;36m(EngineCore_DP0 pid=747894)[0;0m     ^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[0;36m(EngineCore_DP0 pid=747894)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=747894)[0;0m                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=747894)[0;0m         vllm_config
[0;36m(EngineCore_DP0 pid=747894)[0;0m         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m     )
[0;36m(EngineCore_DP0 pid=747894)[0;0m     ^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 242, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=747894)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=747894)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=747894)[0;0m            ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=747894)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=747894)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=747894)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/worker/gpu_worker.py", line 322, in determine_available_memory
[0;36m(EngineCore_DP0 pid=747894)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=747894)[0;0m     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4974, in profile_run
[0;36m(EngineCore_DP0 pid=747894)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=747894)[0;0m                                         ~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=747894)[0;0m         self.max_num_tokens, is_profile=True
[0;36m(EngineCore_DP0 pid=747894)[0;0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m     )
[0;36m(EngineCore_DP0 pid=747894)[0;0m     ^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=747894)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4685, in _dummy_run
[0;36m(EngineCore_DP0 pid=747894)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=747894)[0;0m         input_ids=input_ids,
[0;36m(EngineCore_DP0 pid=747894)[0;0m     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=747894)[0;0m         **model_kwargs,
[0;36m(EngineCore_DP0 pid=747894)[0;0m     )
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/compilation/cuda_graph.py", line 222, in __call__
[0;36m(EngineCore_DP0 pid=747894)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=747894)[0;0m            ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=747894)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=747894)[0;0m            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=747894)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=747894)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=747894)[0;0m         input_ids, positions, intermediate_tensors, inputs_embeds
[0;36m(EngineCore_DP0 pid=747894)[0;0m     )
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/compilation/decorators.py", line 561, in __call__
[0;36m(EngineCore_DP0 pid=747894)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)  # type: ignore[arg-type]
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/compilation/wrapper.py", line 228, in __call__
[0;36m(EngineCore_DP0 pid=747894)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=747894)[0;0m            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=747894)[0;0m         self._compiled_callable, *args, **kwargs
[0;36m(EngineCore_DP0 pid=747894)[0;0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m     )
[0;36m(EngineCore_DP0 pid=747894)[0;0m     ^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/compilation/wrapper.py", line 119, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=747894)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=747894)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=747894)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=747894)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=747894)[0;0m         e.__traceback__
[0;36m(EngineCore_DP0 pid=747894)[0;0m     ) from None
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=747894)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=747894)[0;0m         gm, example_inputs, inputs_to_check, **graph_kwargs
[0;36m(EngineCore_DP0 pid=747894)[0;0m     )
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=747894)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=747894)[0;0m            ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=747894)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=747894)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=747894)[0;0m            ~~~~~~~~~~~~~~~~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=747894)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=747894)[0;0m                                                              ~~~~~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/graph.py", line 2260, in codegen
[0;36m(EngineCore_DP0 pid=747894)[0;0m     self._update_scheduler()
[0;36m(EngineCore_DP0 pid=747894)[0;0m     ~~~~~~~~~~~~~~~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/graph.py", line 2254, in _update_scheduler
[0;36m(EngineCore_DP0 pid=747894)[0;0m     self.scheduler = Scheduler(self.operations)
[0;36m(EngineCore_DP0 pid=747894)[0;0m                      ~~~~~~~~~^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/scheduler.py", line 2226, in __init__
[0;36m(EngineCore_DP0 pid=747894)[0;0m     self._init(nodes)
[0;36m(EngineCore_DP0 pid=747894)[0;0m     ~~~~~~~~~~^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/scheduler.py", line 2320, in _init
[0;36m(EngineCore_DP0 pid=747894)[0;0m     self.create_combo_kernel_nodes(num_ck_nodes=None)
[0;36m(EngineCore_DP0 pid=747894)[0;0m     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/scheduler.py", line 3621, in create_combo_kernel_nodes
[0;36m(EngineCore_DP0 pid=747894)[0;0m     if not self.speedup_by_combo_kernel(node_list):
[0;36m(EngineCore_DP0 pid=747894)[0;0m            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/scheduler.py", line 5509, in speedup_by_combo_kernel
[0;36m(EngineCore_DP0 pid=747894)[0;0m     ms, path = self.benchmark_fused_nodes(node_list)
[0;36m(EngineCore_DP0 pid=747894)[0;0m                ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/scheduler.py", line 3023, in benchmark_fused_nodes
[0;36m(EngineCore_DP0 pid=747894)[0;0m     return backend.benchmark_fused_nodes(nodes)
[0;36m(EngineCore_DP0 pid=747894)[0;0m            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/codegen/cuda_combined_scheduling.py", line 141, in benchmark_fused_nodes
[0;36m(EngineCore_DP0 pid=747894)[0;0m     return self._triton_scheduling.benchmark_fused_nodes(nodes)
[0;36m(EngineCore_DP0 pid=747894)[0;0m            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/codegen/triton.py", line 4778, in benchmark_fused_nodes
[0;36m(EngineCore_DP0 pid=747894)[0;0m     src_code = self.generate_kernel_code_from_nodes(nodes, benchmark_kernel=True)
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/codegen/simd.py", line 2576, in generate_kernel_code_from_nodes
[0;36m(EngineCore_DP0 pid=747894)[0;0m     src_code = kernel.codegen_kernel()
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/codegen/triton.py", line 4173, in codegen_kernel
[0;36m(EngineCore_DP0 pid=747894)[0;0m     **self.inductor_meta_common(),
[0;36m(EngineCore_DP0 pid=747894)[0;0m       ~~~~~~~~~~~~~~~~~~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/_inductor/codegen/triton.py", line 3992, in inductor_meta_common
[0;36m(EngineCore_DP0 pid=747894)[0;0m     "backend_hash": torch.utils._triton.triton_hash_with_backend(),
[0;36m(EngineCore_DP0 pid=747894)[0;0m                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/utils/_triton.py", line 175, in triton_hash_with_backend
[0;36m(EngineCore_DP0 pid=747894)[0;0m     backend = triton_backend()
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/torch/utils/_triton.py", line 167, in triton_backend
[0;36m(EngineCore_DP0 pid=747894)[0;0m     target = driver.active.get_current_target()
[0;36m(EngineCore_DP0 pid=747894)[0;0m              ^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/triton/runtime/driver.py", line 28, in active
[0;36m(EngineCore_DP0 pid=747894)[0;0m     self._active = self.default
[0;36m(EngineCore_DP0 pid=747894)[0;0m                    ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/triton/runtime/driver.py", line 22, in default
[0;36m(EngineCore_DP0 pid=747894)[0;0m     self._default = _create_driver()
[0;36m(EngineCore_DP0 pid=747894)[0;0m                     ~~~~~~~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/triton/runtime/driver.py", line 10, in _create_driver
[0;36m(EngineCore_DP0 pid=747894)[0;0m     return active_drivers[0]()
[0;36m(EngineCore_DP0 pid=747894)[0;0m            ~~~~~~~~~~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/triton/backends/nvidia/driver.py", line 719, in __init__
[0;36m(EngineCore_DP0 pid=747894)[0;0m     self.utils = CudaUtils()  # TODO: make static
[0;36m(EngineCore_DP0 pid=747894)[0;0m                  ~~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/triton/backends/nvidia/driver.py", line 63, in __init__
[0;36m(EngineCore_DP0 pid=747894)[0;0m     mod = compile_module_from_src(
[0;36m(EngineCore_DP0 pid=747894)[0;0m         src=Path(os.path.join(dirname, "driver.c")).read_text(),
[0;36m(EngineCore_DP0 pid=747894)[0;0m     ...<3 lines>...
[0;36m(EngineCore_DP0 pid=747894)[0;0m         libraries=libraries,
[0;36m(EngineCore_DP0 pid=747894)[0;0m     )
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/triton/runtime/build.py", line 89, in compile_module_from_src
[0;36m(EngineCore_DP0 pid=747894)[0;0m     so = _build(name, src_path, tmpdir, library_dirs or [], include_dirs or [], libraries or [], ccflags or [])
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/triton/runtime/build.py", line 51, in _build
[0;36m(EngineCore_DP0 pid=747894)[0;0m     subprocess.check_call(cc_cmd, stdout=subprocess.DEVNULL)
[0;36m(EngineCore_DP0 pid=747894)[0;0m     ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747894)[0;0m   File "/usr/lib/python3.13/subprocess.py", line 419, in check_call
[0;36m(EngineCore_DP0 pid=747894)[0;0m     raise CalledProcessError(retcode, cmd)
[0;36m(EngineCore_DP0 pid=747894)[0;0m torch._inductor.exc.InductorError: CalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmp3x_ljhrt/cuda_utils.c', '-O3', '-shared', '-fPIC', '-Wno-psabi', '-o', '/tmp/tmp3x_ljhrt/cuda_utils.cpython-313-x86_64-linux-gnu.so', '-lcuda', '-L/home/zodrak/aithrt/lib/python3.13/site-packages/triton/backends/nvidia/lib', '-L/lib/x86_64-linux-gnu', '-I/home/zodrak/aithrt/lib/python3.13/site-packages/triton/backends/nvidia/include', '-I/tmp/tmp3x_ljhrt', '-I/usr/include/python3.13']' returned non-zero exit status 1.
[rank0]:[W204 11:55:35.393269786 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[0;36m(APIServer pid=747763)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=747763)[0;0m   File "/home/zodrak/aithrt/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=747763)[0;0m     sys.exit(main())
[0;36m(APIServer pid=747763)[0;0m              ~~~~^^
[0;36m(APIServer pid=747763)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=747763)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=747763)[0;0m     ~~~~~~~~~~~~~~~~~~~~~~^^^^^^
[0;36m(APIServer pid=747763)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=747763)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=747763)[0;0m     ~~~~~~~~~~^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=747763)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/uvloop/__init__.py", line 96, in run
[0;36m(APIServer pid=747763)[0;0m     return __asyncio.run(
[0;36m(APIServer pid=747763)[0;0m            ~~~~~~~~~~~~~^
[0;36m(APIServer pid=747763)[0;0m         wrapper(),
[0;36m(APIServer pid=747763)[0;0m         ^^^^^^^^^^
[0;36m(APIServer pid=747763)[0;0m     ...<2 lines>...
[0;36m(APIServer pid=747763)[0;0m         **run_kwargs
[0;36m(APIServer pid=747763)[0;0m         ^^^^^^^^^^^^
[0;36m(APIServer pid=747763)[0;0m     )
[0;36m(APIServer pid=747763)[0;0m     ^
[0;36m(APIServer pid=747763)[0;0m   File "/usr/lib/python3.13/asyncio/runners.py", line 195, in run
[0;36m(APIServer pid=747763)[0;0m     return runner.run(main)
[0;36m(APIServer pid=747763)[0;0m            ~~~~~~~~~~^^^^^^
[0;36m(APIServer pid=747763)[0;0m   File "/usr/lib/python3.13/asyncio/runners.py", line 118, in run
[0;36m(APIServer pid=747763)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=747763)[0;0m            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
[0;36m(APIServer pid=747763)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=747763)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=747763)[0;0m     return await main
[0;36m(APIServer pid=747763)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=747763)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=747763)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=747763)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=747763)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=747763)[0;0m                ~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(APIServer pid=747763)[0;0m         args,
[0;36m(APIServer pid=747763)[0;0m         ^^^^^
[0;36m(APIServer pid=747763)[0;0m         client_config=client_config,
[0;36m(APIServer pid=747763)[0;0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=747763)[0;0m     ) as engine_client:
[0;36m(APIServer pid=747763)[0;0m     ^
[0;36m(APIServer pid=747763)[0;0m   File "/usr/lib/python3.13/contextlib.py", line 214, in __aenter__
[0;36m(APIServer pid=747763)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=747763)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=747763)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=747763)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=747763)[0;0m                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(APIServer pid=747763)[0;0m         engine_args,
[0;36m(APIServer pid=747763)[0;0m         ^^^^^^^^^^^^
[0;36m(APIServer pid=747763)[0;0m     ...<2 lines>...
[0;36m(APIServer pid=747763)[0;0m         client_config=client_config,
[0;36m(APIServer pid=747763)[0;0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=747763)[0;0m     ) as engine:
[0;36m(APIServer pid=747763)[0;0m     ^
[0;36m(APIServer pid=747763)[0;0m   File "/usr/lib/python3.13/contextlib.py", line 214, in __aenter__
[0;36m(APIServer pid=747763)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=747763)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=747763)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=747763)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=747763)[0;0m         vllm_config=vllm_config,
[0;36m(APIServer pid=747763)[0;0m     ...<6 lines>...
[0;36m(APIServer pid=747763)[0;0m         client_index=client_index,
[0;36m(APIServer pid=747763)[0;0m     )
[0;36m(APIServer pid=747763)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=747763)[0;0m     return cls(
[0;36m(APIServer pid=747763)[0;0m         vllm_config=vllm_config,
[0;36m(APIServer pid=747763)[0;0m     ...<9 lines>...
[0;36m(APIServer pid=747763)[0;0m         client_index=client_index,
[0;36m(APIServer pid=747763)[0;0m     )
[0;36m(APIServer pid=747763)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=747763)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=747763)[0;0m                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(APIServer pid=747763)[0;0m         vllm_config=vllm_config,
[0;36m(APIServer pid=747763)[0;0m         ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=747763)[0;0m     ...<4 lines>...
[0;36m(APIServer pid=747763)[0;0m         client_index=client_index,
[0;36m(APIServer pid=747763)[0;0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=747763)[0;0m     )
[0;36m(APIServer pid=747763)[0;0m     ^
[0;36m(APIServer pid=747763)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=747763)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=747763)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=747763)[0;0m     super().__init__(
[0;36m(APIServer pid=747763)[0;0m     ~~~~~~~~~~~~~~~~^
[0;36m(APIServer pid=747763)[0;0m         asyncio_mode=True,
[0;36m(APIServer pid=747763)[0;0m         ^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=747763)[0;0m     ...<3 lines>...
[0;36m(APIServer pid=747763)[0;0m         client_addresses=client_addresses,
[0;36m(APIServer pid=747763)[0;0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=747763)[0;0m     )
[0;36m(APIServer pid=747763)[0;0m     ^
[0;36m(APIServer pid=747763)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=747763)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=747763)[0;0m          ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=747763)[0;0m   File "/usr/lib/python3.13/contextlib.py", line 148, in __exit__
[0;36m(APIServer pid=747763)[0;0m     next(self.gen)
[0;36m(APIServer pid=747763)[0;0m     ~~~~^^^^^^^^^^
[0;36m(APIServer pid=747763)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=747763)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=747763)[0;0m     ~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(APIServer pid=747763)[0;0m         handshake_socket,
[0;36m(APIServer pid=747763)[0;0m         ^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=747763)[0;0m     ...<6 lines>...
[0;36m(APIServer pid=747763)[0;0m         coordinator.proc if coordinator else None,
[0;36m(APIServer pid=747763)[0;0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=747763)[0;0m     )
[0;36m(APIServer pid=747763)[0;0m     ^
[0;36m(APIServer pid=747763)[0;0m   File "/home/zodrak/aithrt/lib/python3.13/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=747763)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=747763)[0;0m     ...<3 lines>...
[0;36m(APIServer pid=747763)[0;0m     )
[0;36m(APIServer pid=747763)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
/home/zodrak/.pyenv/versions/py312/bin/python: Error while finding module specification for 'vllm.entrypoints.openai.api_server' (ModuleNotFoundError: No module named 'vllm')
[0;36m(APIServer pid=787983)[0;0m INFO 02-04 15:32:19 [utils.py:325] 
[0;36m(APIServer pid=787983)[0;0m INFO 02-04 15:32:19 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=787983)[0;0m INFO 02-04 15:32:19 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
[0;36m(APIServer pid=787983)[0;0m INFO 02-04 15:32:19 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   Qwen/Qwen2.5-3B-Instruct-AWQ
[0;36m(APIServer pid=787983)[0;0m INFO 02-04 15:32:19 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=787983)[0;0m INFO 02-04 15:32:19 [utils.py:325] 
[0;36m(APIServer pid=787983)[0;0m INFO 02-04 15:32:19 [utils.py:261] non-default args: {'model_tag': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'api_server_count': 1, 'host': '0.0.0.0', 'model': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'max_model_len': 8192, 'quantization': 'awq', 'gpu_memory_utilization': 0.5}
[0;36m(APIServer pid=787983)[0;0m INFO 02-04 15:32:20 [model.py:541] Resolved architecture: Qwen2ForCausalLM
[0;36m(APIServer pid=787983)[0;0m INFO 02-04 15:32:20 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=787983)[0;0m INFO 02-04 15:32:20 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference
[0;36m(APIServer pid=787983)[0;0m INFO 02-04 15:32:20 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=787983)[0;0m INFO 02-04 15:32:21 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=788891)[0;0m INFO 02-04 15:32:28 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='Qwen/Qwen2.5-3B-Instruct-AWQ', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen2.5-3B-Instruct-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=788891)[0;0m INFO 02-04 15:32:29 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.0.76:56291 backend=nccl
[0;36m(EngineCore_DP0 pid=788891)[0;0m INFO 02-04 15:32:29 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=788891)[0;0m INFO 02-04 15:32:29 [gpu_model_runner.py:4021] Starting to load model Qwen/Qwen2.5-3B-Instruct-AWQ...
[0;36m(EngineCore_DP0 pid=788891)[0;0m INFO 02-04 15:32:44 [cuda.py:364] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=788891)[0;0m INFO 02-04 15:32:45 [weight_utils.py:567] No model.safetensors.index.json found in remote.
[0;36m(EngineCore_DP0 pid=788891)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=788891)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.29s/it]
[0;36m(EngineCore_DP0 pid=788891)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.29s/it]
[0;36m(EngineCore_DP0 pid=788891)[0;0m 
[0;36m(EngineCore_DP0 pid=788891)[0;0m INFO 02-04 15:32:46 [default_loader.py:291] Loading weights took 1.29 seconds
[0;36m(EngineCore_DP0 pid=788891)[0;0m INFO 02-04 15:32:46 [gpu_model_runner.py:4118] Model loading took 1.95 GiB memory and 16.743206 seconds
[0;36m(EngineCore_DP0 pid=788891)[0;0m INFO 02-04 15:32:54 [backends.py:805] Using cache directory: /home/zodrak/.cache/vllm/torch_compile_cache/4d19c4f626/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=788891)[0;0m INFO 02-04 15:32:54 [backends.py:865] Dynamo bytecode transform time: 7.36 s
[0;36m(EngineCore_DP0 pid=788891)[0;0m INFO 02-04 15:33:05 [backends.py:302] Cache the graph of compile range (1, 2048) for later use
[0;36m(EngineCore_DP0 pid=788891)[0;0m [rank0]:W0204 15:33:06.009000 788891 torch/_inductor/utils.py:1613] Not enough SMs to use max_autotune_gemm mode
[rank0]:[W204 15:33:16.115205507 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[0;36m(APIServer pid=787983)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=787983)[0;0m   File "/home/zodrak/.pyenv/versions/py312/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=787983)[0;0m     sys.exit(main())
[0;36m(APIServer pid=787983)[0;0m              ^^^^^^
[0;36m(APIServer pid=787983)[0;0m   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=787983)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=787983)[0;0m   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=787983)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=787983)[0;0m   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[0;36m(APIServer pid=787983)[0;0m     return __asyncio.run(
[0;36m(APIServer pid=787983)[0;0m            ^^^^^^^^^^^^^^
[0;36m(APIServer pid=787983)[0;0m   File "/home/zodrak/.pyenv/versions/3.12.8/lib/python3.12/asyncio/runners.py", line 194, in run
[0;36m(APIServer pid=787983)[0;0m     return runner.run(main)
[0;36m(APIServer pid=787983)[0;0m            ^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=787983)[0;0m   File "/home/zodrak/.pyenv/versions/3.12.8/lib/python3.12/asyncio/runners.py", line 118, in run
[0;36m(APIServer pid=787983)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=787983)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=787983)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=787983)[0;0m   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=787983)[0;0m     return await main
[0;36m(APIServer pid=787983)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=787983)[0;0m   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=787983)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=787983)[0;0m   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=787983)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=787983)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=787983)[0;0m   File "/home/zodrak/.pyenv/versions/3.12.8/lib/python3.12/contextlib.py", line 210, in __aenter__
[0;36m(APIServer pid=787983)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=787983)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=787983)[0;0m   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=787983)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=787983)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=787983)[0;0m   File "/home/zodrak/.pyenv/versions/3.12.8/lib/python3.12/contextlib.py", line 210, in __aenter__
[0;36m(APIServer pid=787983)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=787983)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=787983)[0;0m   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=787983)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=787983)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=787983)[0;0m   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=787983)[0;0m     return cls(
[0;36m(APIServer pid=787983)[0;0m            ^^^^
[0;36m(APIServer pid=787983)[0;0m   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=787983)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=787983)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=787983)[0;0m   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=787983)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=787983)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=787983)[0;0m   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=787983)[0;0m     super().__init__(
[0;36m(APIServer pid=787983)[0;0m   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=787983)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=787983)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=787983)[0;0m   File "/home/zodrak/.pyenv/versions/3.12.8/lib/python3.12/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=787983)[0;0m     next(self.gen)
[0;36m(APIServer pid=787983)[0;0m   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=787983)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=787983)[0;0m   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=787983)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=787983)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:36:02 [utils.py:325] 
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:36:02 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:36:02 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:36:02 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   Qwen/Qwen2.5-3B-Instruct-AWQ
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:36:02 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:36:02 [utils.py:325] 
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:36:02 [utils.py:261] non-default args: {'model_tag': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'api_server_count': 1, 'host': '0.0.0.0', 'model': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'max_model_len': 8192, 'quantization': 'awq', 'gpu_memory_utilization': 0.5}
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:36:03 [model.py:541] Resolved architecture: Qwen2ForCausalLM
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:36:03 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:36:03 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:36:03 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:36:04 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=793572)[0;0m INFO 02-04 15:36:11 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='Qwen/Qwen2.5-3B-Instruct-AWQ', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen2.5-3B-Instruct-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=793572)[0;0m INFO 02-04 15:36:11 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.0.76:57295 backend=nccl
[0;36m(EngineCore_DP0 pid=793572)[0;0m INFO 02-04 15:36:12 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=793572)[0;0m INFO 02-04 15:36:12 [gpu_model_runner.py:4021] Starting to load model Qwen/Qwen2.5-3B-Instruct-AWQ...
[0;36m(EngineCore_DP0 pid=793572)[0;0m INFO 02-04 15:36:13 [cuda.py:364] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=793572)[0;0m INFO 02-04 15:36:14 [weight_utils.py:567] No model.safetensors.index.json found in remote.
[0;36m(EngineCore_DP0 pid=793572)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=793572)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.39it/s]
[0;36m(EngineCore_DP0 pid=793572)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.39it/s]
[0;36m(EngineCore_DP0 pid=793572)[0;0m 
[0;36m(EngineCore_DP0 pid=793572)[0;0m INFO 02-04 15:36:14 [default_loader.py:291] Loading weights took 0.30 seconds
[0;36m(EngineCore_DP0 pid=793572)[0;0m INFO 02-04 15:36:14 [gpu_model_runner.py:4118] Model loading took 1.95 GiB memory and 1.929437 seconds
[0;36m(EngineCore_DP0 pid=793572)[0;0m INFO 02-04 15:36:21 [backends.py:805] Using cache directory: /home/zodrak/.cache/vllm/torch_compile_cache/4d19c4f626/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=793572)[0;0m INFO 02-04 15:36:21 [backends.py:865] Dynamo bytecode transform time: 6.80 s
[0;36m(EngineCore_DP0 pid=793572)[0;0m INFO 02-04 15:36:28 [backends.py:302] Cache the graph of compile range (1, 2048) for later use
[0;36m(EngineCore_DP0 pid=793572)[0;0m [rank0]:W0204 15:36:30.290000 793572 torch/_inductor/utils.py:1613] Not enough SMs to use max_autotune_gemm mode
[0;36m(EngineCore_DP0 pid=793572)[0;0m INFO 02-04 15:37:02 [backends.py:319] Compiling a graph for compile range (1, 2048) takes 34.15 s
[0;36m(EngineCore_DP0 pid=793572)[0;0m INFO 02-04 15:37:02 [monitor.py:34] torch.compile takes 40.95 s in total
[0;36m(EngineCore_DP0 pid=793572)[0;0m INFO 02-04 15:37:03 [gpu_worker.py:356] Available KV cache memory: 2.43 GiB
[0;36m(EngineCore_DP0 pid=793572)[0;0m INFO 02-04 15:37:03 [kv_cache_utils.py:1307] GPU KV cache size: 70,816 tokens
[0;36m(EngineCore_DP0 pid=793572)[0;0m INFO 02-04 15:37:03 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 8.64x
[0;36m(EngineCore_DP0 pid=793572)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|â–         | 1/51 [00:00<00:12,  4.11it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 2/51 [00:00<00:11,  4.26it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 3/51 [00:00<00:10,  4.67it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 4/51 [00:00<00:09,  4.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|â–‰         | 5/51 [00:01<00:09,  4.97it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:01<00:08,  5.07it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|â–ˆâ–Ž        | 7/51 [00:01<00:08,  5.14it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 8/51 [00:01<00:08,  5.19it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 9/51 [00:01<00:08,  5.24it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–‰        | 10/51 [00:01<00:07,  5.25it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 11/51 [00:02<00:07,  5.23it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:02<00:07,  5.23it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:02<00:06,  5.44it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:02<00:06,  5.60it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:02<00:06,  5.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:03<00:06,  5.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:03<00:05,  6.20it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:03<00:05,  6.53it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:03<00:04,  6.81it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:03<00:04,  7.02it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:03<00:04,  7.14it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:03<00:03,  7.27it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:03<00:03,  7.34it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:04<00:03,  7.41it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:04<00:03,  7.68it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:04<00:03,  7.57it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:04<00:03,  7.51it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:04<00:03,  7.47it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:04<00:02,  7.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:04<00:02,  8.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:04<00:02,  8.15it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:05<00:02,  8.19it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:05<00:02,  8.55it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:05<00:01,  8.84it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:05<00:01,  9.01it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:05<00:01,  9.19it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [00:05<00:01,  9.06it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [00:05<00:01,  9.59it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [00:06<00:01,  9.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 43/51 [00:06<00:00, 10.44it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 45/51 [00:06<00:00, 10.76it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [00:06<00:00, 10.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/51 [00:06<00:00, 11.13it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:06<00:00, 11.25it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:06<00:00,  7.40it/s]
[0;36m(EngineCore_DP0 pid=793572)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   3%|â–Ž         | 1/35 [00:00<00:05,  6.43it/s]Capturing CUDA graphs (decode, FULL):   6%|â–Œ         | 2/35 [00:00<00:04,  6.88it/s]Capturing CUDA graphs (decode, FULL):   9%|â–Š         | 3/35 [00:00<00:04,  7.05it/s]Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:04,  7.15it/s]Capturing CUDA graphs (decode, FULL):  14%|â–ˆâ–        | 5/35 [00:00<00:04,  7.21it/s]Capturing CUDA graphs (decode, FULL):  17%|â–ˆâ–‹        | 6/35 [00:00<00:03,  7.28it/s]Capturing CUDA graphs (decode, FULL):  20%|â–ˆâ–ˆ        | 7/35 [00:00<00:03,  7.33it/s]Capturing CUDA graphs (decode, FULL):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:01<00:03,  7.39it/s]Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:01<00:03,  7.63it/s]Capturing CUDA graphs (decode, FULL):  29%|â–ˆâ–ˆâ–Š       | 10/35 [00:01<00:03,  7.52it/s]Capturing CUDA graphs (decode, FULL):  31%|â–ˆâ–ˆâ–ˆâ–      | 11/35 [00:01<00:03,  7.48it/s]Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:01<00:03,  7.46it/s]Capturing CUDA graphs (decode, FULL):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [00:01<00:02,  7.78it/s]Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:01<00:02,  8.05it/s]Capturing CUDA graphs (decode, FULL):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15/35 [00:01<00:02,  8.24it/s]Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:02<00:02,  8.24it/s]Capturing CUDA graphs (decode, FULL):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [00:02<00:02,  8.52it/s]Capturing CUDA graphs (decode, FULL):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:02<00:01,  8.77it/s]Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:02<00:01,  8.92it/s]Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:02<00:01,  9.08it/s]Capturing CUDA graphs (decode, FULL):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:02<00:01,  9.19it/s]Capturing CUDA graphs (decode, FULL):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 23/35 [00:02<00:01,  9.65it/s]Capturing CUDA graphs (decode, FULL):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25/35 [00:03<00:00, 10.11it/s]Capturing CUDA graphs (decode, FULL):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 27/35 [00:03<00:00, 10.55it/s]Capturing CUDA graphs (decode, FULL):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 29/35 [00:03<00:00, 10.93it/s]Capturing CUDA graphs (decode, FULL):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 31/35 [00:03<00:00, 11.16it/s]Capturing CUDA graphs (decode, FULL):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 33/35 [00:03<00:00, 11.39it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:03<00:00, 11.62it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:03<00:00,  9.06it/s]
[0;36m(EngineCore_DP0 pid=793572)[0;0m INFO 02-04 15:37:14 [gpu_model_runner.py:5051] Graph capturing finished in 11 secs, took 0.72 GiB
[0;36m(EngineCore_DP0 pid=793572)[0;0m INFO 02-04 15:37:14 [core.py:272] init engine (profile, create kv cache, warmup model) took 59.92 seconds
[0;36m(EngineCore_DP0 pid=793572)[0;0m INFO 02-04 15:37:16 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:16 [api_server.py:665] Supported tasks: ['generate']
[0;36m(APIServer pid=792715)[0;0m WARNING 02-04 15:37:16 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:16 [serving.py:177] Warming up chat template processing...
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [hf.py:310] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [serving.py:212] Chat template warmup completed in 1560.7ms
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:37:18 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=792715)[0;0m INFO:     Started server process [792715]
[0;36m(APIServer pid=792715)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=792715)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=792715)[0;0m ERROR 02-04 15:39:14 [serving.py:231] Error with model error=ErrorInfo(message='The model `Qwen/Qwen2.5-3B-Instruct` does not exist.', type='NotFoundError', param='model', code=404)
[0;36m(APIServer pid=792715)[0;0m INFO:     127.0.0.1:45640 - "POST /v1/chat/completions HTTP/1.1" 404 Not Found
[0;36m(APIServer pid=792715)[0;0m ERROR 02-04 15:41:57 [serving.py:231] Error with model error=ErrorInfo(message='The model `Qwen/Qwen2.5-3B-Instruct` does not exist.', type='NotFoundError', param='model', code=404)
[0;36m(APIServer pid=792715)[0;0m INFO:     127.0.0.1:48046 - "POST /v1/chat/completions HTTP/1.1" 404 Not Found
[0;36m(APIServer pid=792715)[0;0m INFO:     127.0.0.1:41350 - "GET /v1/models HTTP/1.1" 200 OK
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:45:48 [loggers.py:257] Engine 000: Avg prompt throughput: 64.8 tokens/s, Avg generation throughput: 9.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:45:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:46:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:46:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:46:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:46:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:46:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:46:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=792715)[0;0m INFO 02-04 15:55:44 [launcher.py:110] Shutting down FastAPI HTTP server.
[0;36m(APIServer pid=792715)[0;0m INFO:     Shutting down
[0;36m(APIServer pid=792715)[0;0m INFO:     Waiting for application shutdown.
[0;36m(APIServer pid=792715)[0;0m INFO:     Application shutdown complete.
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:55:56 [utils.py:325] 
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:55:56 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:55:56 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:55:56 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   Qwen/Qwen2.5-3B-Instruct-AWQ
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:55:56 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:55:56 [utils.py:325] 
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:55:56 [utils.py:261] non-default args: {'model_tag': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'api_server_count': 1, 'host': '0.0.0.0', 'model': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'max_model_len': 8192, 'quantization': 'awq', 'gpu_memory_utilization': 0.5}
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:55:57 [model.py:541] Resolved architecture: Qwen2ForCausalLM
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:55:57 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:55:57 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:55:57 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:55:58 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=805950)[0;0m INFO 02-04 15:56:05 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='Qwen/Qwen2.5-3B-Instruct-AWQ', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen2.5-3B-Instruct-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=805950)[0;0m INFO 02-04 15:56:06 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.0.76:41339 backend=nccl
[0;36m(EngineCore_DP0 pid=805950)[0;0m INFO 02-04 15:56:06 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=805950)[0;0m INFO 02-04 15:56:06 [gpu_model_runner.py:4021] Starting to load model Qwen/Qwen2.5-3B-Instruct-AWQ...
[0;36m(EngineCore_DP0 pid=805950)[0;0m INFO 02-04 15:56:07 [cuda.py:364] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=805950)[0;0m INFO 02-04 15:56:07 [weight_utils.py:567] No model.safetensors.index.json found in remote.
[0;36m(EngineCore_DP0 pid=805950)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=805950)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.33it/s]
[0;36m(EngineCore_DP0 pid=805950)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.33it/s]
[0;36m(EngineCore_DP0 pid=805950)[0;0m 
[0;36m(EngineCore_DP0 pid=805950)[0;0m INFO 02-04 15:56:08 [default_loader.py:291] Loading weights took 0.31 seconds
[0;36m(EngineCore_DP0 pid=805950)[0;0m INFO 02-04 15:56:08 [gpu_model_runner.py:4118] Model loading took 1.95 GiB memory and 1.754106 seconds
[0;36m(EngineCore_DP0 pid=805950)[0;0m INFO 02-04 15:56:15 [backends.py:805] Using cache directory: /home/zodrak/.cache/vllm/torch_compile_cache/4d19c4f626/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=805950)[0;0m INFO 02-04 15:56:15 [backends.py:865] Dynamo bytecode transform time: 6.73 s
[0;36m(EngineCore_DP0 pid=805950)[0;0m INFO 02-04 15:56:23 [backends.py:267] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 0.757 s
[0;36m(EngineCore_DP0 pid=805950)[0;0m INFO 02-04 15:56:23 [monitor.py:34] torch.compile takes 7.49 s in total
[0;36m(EngineCore_DP0 pid=805950)[0;0m INFO 02-04 15:56:23 [gpu_worker.py:356] Available KV cache memory: 2.43 GiB
[0;36m(EngineCore_DP0 pid=805950)[0;0m INFO 02-04 15:56:23 [kv_cache_utils.py:1307] GPU KV cache size: 70,768 tokens
[0;36m(EngineCore_DP0 pid=805950)[0;0m INFO 02-04 15:56:23 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 8.64x
[0;36m(EngineCore_DP0 pid=805950)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|â–         | 1/51 [00:00<00:11,  4.29it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 2/51 [00:00<00:11,  4.33it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 3/51 [00:00<00:10,  4.67it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 4/51 [00:00<00:09,  4.85it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|â–‰         | 5/51 [00:01<00:09,  4.97it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:01<00:08,  5.05it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|â–ˆâ–Ž        | 7/51 [00:01<00:08,  5.12it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 8/51 [00:01<00:08,  5.17it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 9/51 [00:01<00:08,  5.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–‰        | 10/51 [00:01<00:07,  5.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 11/51 [00:02<00:07,  5.24it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:02<00:07,  5.29it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:02<00:06,  5.51it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:02<00:06,  5.70it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:02<00:06,  5.83it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:03<00:05,  5.89it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:03<00:05,  6.28it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:03<00:04,  6.62it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:03<00:04,  6.87it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:03<00:04,  7.02it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:03<00:04,  7.18it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:03<00:03,  7.33it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:03<00:03,  7.42it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:04<00:03,  7.52it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:04<00:03,  7.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:04<00:03,  7.67it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:04<00:03,  7.56it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:04<00:03,  7.48it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:04<00:02,  7.82it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:04<00:02,  8.06it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:04<00:02,  8.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:05<00:02,  8.30it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:05<00:02,  8.60it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:05<00:01,  8.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:05<00:01,  9.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:05<00:01,  9.13it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [00:05<00:01,  9.04it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [00:05<00:01,  9.58it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [00:05<00:01,  9.99it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 43/51 [00:06<00:00, 10.48it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 45/51 [00:06<00:00, 10.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [00:06<00:00, 10.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/51 [00:06<00:00, 11.09it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:06<00:00, 11.24it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:06<00:00,  7.43it/s]
[0;36m(EngineCore_DP0 pid=805950)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   3%|â–Ž         | 1/35 [00:00<00:05,  6.74it/s]Capturing CUDA graphs (decode, FULL):   6%|â–Œ         | 2/35 [00:00<00:04,  7.12it/s]Capturing CUDA graphs (decode, FULL):   9%|â–Š         | 3/35 [00:00<00:04,  7.28it/s]Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:04,  7.36it/s]Capturing CUDA graphs (decode, FULL):  14%|â–ˆâ–        | 5/35 [00:00<00:04,  7.34it/s]Capturing CUDA graphs (decode, FULL):  17%|â–ˆâ–‹        | 6/35 [00:00<00:03,  7.44it/s]Capturing CUDA graphs (decode, FULL):  20%|â–ˆâ–ˆ        | 7/35 [00:00<00:03,  7.47it/s]Capturing CUDA graphs (decode, FULL):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:01<00:03,  7.54it/s]Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:01<00:03,  7.81it/s]Capturing CUDA graphs (decode, FULL):  29%|â–ˆâ–ˆâ–Š       | 10/35 [00:01<00:03,  7.66it/s]Capturing CUDA graphs (decode, FULL):  31%|â–ˆâ–ˆâ–ˆâ–      | 11/35 [00:01<00:03,  7.57it/s]Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:01<00:03,  7.48it/s]Capturing CUDA graphs (decode, FULL):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [00:01<00:02,  7.76it/s]Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:01<00:02,  8.04it/s]Capturing CUDA graphs (decode, FULL):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15/35 [00:01<00:02,  8.08it/s]Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:02<00:02,  8.18it/s]Capturing CUDA graphs (decode, FULL):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [00:02<00:02,  8.58it/s]Capturing CUDA graphs (decode, FULL):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:02<00:01,  8.84it/s]Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:02<00:01,  9.03it/s]Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:02<00:01,  9.21it/s]Capturing CUDA graphs (decode, FULL):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:02<00:01,  9.28it/s]Capturing CUDA graphs (decode, FULL):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 23/35 [00:02<00:01,  9.72it/s]Capturing CUDA graphs (decode, FULL):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25/35 [00:02<00:00, 10.14it/s]Capturing CUDA graphs (decode, FULL):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 27/35 [00:03<00:00, 10.62it/s]Capturing CUDA graphs (decode, FULL):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 29/35 [00:03<00:00, 10.93it/s]Capturing CUDA graphs (decode, FULL):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 31/35 [00:03<00:00, 11.16it/s]Capturing CUDA graphs (decode, FULL):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 33/35 [00:03<00:00, 11.31it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:03<00:00, 11.53it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:03<00:00,  9.12it/s]
[0;36m(EngineCore_DP0 pid=805950)[0;0m INFO 02-04 15:56:35 [gpu_model_runner.py:5051] Graph capturing finished in 11 secs, took 0.76 GiB
[0;36m(EngineCore_DP0 pid=805950)[0;0m INFO 02-04 15:56:35 [core.py:272] init engine (profile, create kv cache, warmup model) took 26.41 seconds
[0;36m(EngineCore_DP0 pid=805950)[0;0m INFO 02-04 15:56:36 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:36 [api_server.py:665] Supported tasks: ['generate']
[0;36m(APIServer pid=805051)[0;0m WARNING 02-04 15:56:36 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:36 [serving.py:177] Warming up chat template processing...
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [hf.py:310] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [serving.py:212] Chat template warmup completed in 1612.5ms
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:56:38 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=805051)[0;0m INFO:     Started server process [805051]
[0;36m(APIServer pid=805051)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=805051)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=805051)[0;0m ERROR 02-04 15:57:11 [serving.py:231] Error with model error=ErrorInfo(message='The model `Qwen/Qwen2.5-3B-Instruct` does not exist.', type='NotFoundError', param='model', code=404)
[0;36m(APIServer pid=805051)[0;0m INFO:     127.0.0.1:47086 - "POST /v1/chat/completions HTTP/1.1" 404 Not Found
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:58:39 [loggers.py:257] Engine 000: Avg prompt throughput: 64.8 tokens/s, Avg generation throughput: 1.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:58:49 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 15:58:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 16:00:39 [loggers.py:257] Engine 000: Avg prompt throughput: 64.8 tokens/s, Avg generation throughput: 11.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 49.4%
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 16:00:49 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 49.4%
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 16:00:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 49.4%
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 16:01:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 49.4%
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 16:01:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 49.4%
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 16:01:29 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 49.4%
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 16:01:39 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 49.4%
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 16:01:49 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 49.4%
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 17:03:49 [loggers.py:257] Engine 000: Avg prompt throughput: 64.8 tokens/s, Avg generation throughput: 4.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 65.8%
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 17:03:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 65.8%
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 17:04:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 65.8%
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 17:04:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 65.8%
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 17:04:29 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 65.8%
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 17:04:39 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 65.8%
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 17:04:49 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 10.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 65.8%
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 17:04:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 65.8%
[0;36m(APIServer pid=805051)[0;0m INFO 02-04 17:05:20 [launcher.py:110] Shutting down FastAPI HTTP server.
[0;36m(APIServer pid=805051)[0;0m INFO:     Shutting down
[0;36m(APIServer pid=805051)[0;0m INFO:     Waiting for application shutdown.
[0;36m(APIServer pid=805051)[0;0m INFO:     Application shutdown complete.
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:05:43 [utils.py:325] 
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:05:43 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:05:43 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:05:43 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   Qwen/Qwen2.5-3B-Instruct-AWQ
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:05:43 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:05:43 [utils.py:325] 
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:05:43 [utils.py:261] non-default args: {'model_tag': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'api_server_count': 1, 'host': '0.0.0.0', 'model': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'max_model_len': 8192, 'quantization': 'awq', 'gpu_memory_utilization': 0.5}
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:05:44 [model.py:541] Resolved architecture: Qwen2ForCausalLM
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:05:44 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:05:44 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:05:44 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:05:45 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=821903)[0;0m INFO 02-04 17:05:52 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='Qwen/Qwen2.5-3B-Instruct-AWQ', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen2.5-3B-Instruct-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=821903)[0;0m INFO 02-04 17:05:52 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.0.76:54373 backend=nccl
[0;36m(EngineCore_DP0 pid=821903)[0;0m INFO 02-04 17:05:52 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=821903)[0;0m INFO 02-04 17:05:53 [gpu_model_runner.py:4021] Starting to load model Qwen/Qwen2.5-3B-Instruct-AWQ...
[0;36m(EngineCore_DP0 pid=821903)[0;0m INFO 02-04 17:05:53 [cuda.py:364] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=821903)[0;0m INFO 02-04 17:05:55 [weight_utils.py:567] No model.safetensors.index.json found in remote.
[0;36m(EngineCore_DP0 pid=821903)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=821903)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.28it/s]
[0;36m(EngineCore_DP0 pid=821903)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.28it/s]
[0;36m(EngineCore_DP0 pid=821903)[0;0m 
[0;36m(EngineCore_DP0 pid=821903)[0;0m INFO 02-04 17:05:55 [default_loader.py:291] Loading weights took 0.31 seconds
[0;36m(EngineCore_DP0 pid=821903)[0;0m INFO 02-04 17:05:55 [gpu_model_runner.py:4118] Model loading took 1.95 GiB memory and 2.057328 seconds
[0;36m(EngineCore_DP0 pid=821903)[0;0m INFO 02-04 17:06:02 [backends.py:805] Using cache directory: /home/zodrak/.cache/vllm/torch_compile_cache/4d19c4f626/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=821903)[0;0m INFO 02-04 17:06:02 [backends.py:865] Dynamo bytecode transform time: 6.93 s
[0;36m(EngineCore_DP0 pid=821903)[0;0m INFO 02-04 17:06:10 [backends.py:267] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 0.769 s
[0;36m(EngineCore_DP0 pid=821903)[0;0m INFO 02-04 17:06:10 [monitor.py:34] torch.compile takes 7.70 s in total
[0;36m(EngineCore_DP0 pid=821903)[0;0m INFO 02-04 17:06:11 [gpu_worker.py:356] Available KV cache memory: 2.46 GiB
[0;36m(EngineCore_DP0 pid=821903)[0;0m INFO 02-04 17:06:11 [kv_cache_utils.py:1307] GPU KV cache size: 71,696 tokens
[0;36m(EngineCore_DP0 pid=821903)[0;0m INFO 02-04 17:06:11 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 8.75x
[0;36m(EngineCore_DP0 pid=821903)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|â–         | 1/51 [00:00<00:12,  4.14it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 2/51 [00:00<00:11,  4.20it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 3/51 [00:00<00:10,  4.59it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 4/51 [00:00<00:09,  4.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|â–‰         | 5/51 [00:01<00:09,  4.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:01<00:08,  5.02it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|â–ˆâ–Ž        | 7/51 [00:01<00:08,  5.08it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 8/51 [00:01<00:08,  5.13it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 9/51 [00:01<00:08,  5.18it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–‰        | 10/51 [00:02<00:07,  5.22it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 11/51 [00:02<00:07,  5.24it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:02<00:07,  5.22it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:02<00:06,  5.44it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:02<00:06,  5.61it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:02<00:06,  5.73it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:03<00:06,  5.82it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:03<00:05,  6.19it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:03<00:05,  6.48it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:03<00:04,  6.73it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:03<00:04,  6.92it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:03<00:04,  7.06it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:03<00:04,  7.19it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:04<00:03,  7.28it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:04<00:03,  7.35it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:04<00:03,  7.59it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:04<00:03,  7.43it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:04<00:03,  7.37it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:04<00:03,  7.33it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:04<00:02,  7.67it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:04<00:02,  7.91it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:05<00:02,  8.07it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:05<00:02,  8.15it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:05<00:02,  8.44it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:05<00:01,  8.68it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:05<00:01,  8.86it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:05<00:01,  9.04it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [00:05<00:01,  8.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:05<00:01,  9.22it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [00:05<00:01,  9.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [00:06<00:00, 10.18it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:06<00:00, 10.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [00:06<00:00, 10.65it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [00:06<00:00, 10.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [00:06<00:00, 10.76it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:06<00:00,  7.30it/s]
[0;36m(EngineCore_DP0 pid=821903)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   3%|â–Ž         | 1/35 [00:00<00:05,  6.33it/s]Capturing CUDA graphs (decode, FULL):   6%|â–Œ         | 2/35 [00:00<00:04,  6.76it/s]Capturing CUDA graphs (decode, FULL):   9%|â–Š         | 3/35 [00:00<00:04,  6.96it/s]Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:04,  7.08it/s]Capturing CUDA graphs (decode, FULL):  14%|â–ˆâ–        | 5/35 [00:00<00:04,  7.10it/s]Capturing CUDA graphs (decode, FULL):  17%|â–ˆâ–‹        | 6/35 [00:00<00:04,  7.16it/s]Capturing CUDA graphs (decode, FULL):  20%|â–ˆâ–ˆ        | 7/35 [00:00<00:03,  7.15it/s]Capturing CUDA graphs (decode, FULL):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:01<00:03,  7.21it/s]Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:01<00:03,  7.44it/s]Capturing CUDA graphs (decode, FULL):  29%|â–ˆâ–ˆâ–Š       | 10/35 [00:01<00:03,  7.30it/s]Capturing CUDA graphs (decode, FULL):  31%|â–ˆâ–ˆâ–ˆâ–      | 11/35 [00:01<00:03,  7.26it/s]Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:01<00:03,  7.26it/s]Capturing CUDA graphs (decode, FULL):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [00:01<00:02,  7.57it/s]Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:01<00:02,  7.83it/s]Capturing CUDA graphs (decode, FULL):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15/35 [00:02<00:02,  8.02it/s]Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:02<00:02,  8.10it/s]Capturing CUDA graphs (decode, FULL):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [00:02<00:02,  8.43it/s]Capturing CUDA graphs (decode, FULL):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:02<00:01,  8.61it/s]Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:02<00:01,  8.67it/s]Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:02<00:01,  8.77it/s]Capturing CUDA graphs (decode, FULL):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:02<00:01,  8.80it/s]Capturing CUDA graphs (decode, FULL):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 22/35 [00:02<00:01,  9.10it/s]Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:02<00:01,  9.62it/s]Capturing CUDA graphs (decode, FULL):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26/35 [00:03<00:00, 10.10it/s]Capturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:03<00:00, 10.47it/s]Capturing CUDA graphs (decode, FULL):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 30/35 [00:03<00:00, 10.75it/s]Capturing CUDA graphs (decode, FULL):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:03<00:00, 10.98it/s]Capturing CUDA graphs (decode, FULL):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:03<00:00, 11.14it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:03<00:00,  8.81it/s]
[0;36m(EngineCore_DP0 pid=821903)[0;0m INFO 02-04 17:06:22 [gpu_model_runner.py:5051] Graph capturing finished in 11 secs, took 0.71 GiB
[0;36m(EngineCore_DP0 pid=821903)[0;0m INFO 02-04 17:06:23 [core.py:272] init engine (profile, create kv cache, warmup model) took 27.27 seconds
[0;36m(EngineCore_DP0 pid=821903)[0;0m INFO 02-04 17:06:24 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:24 [api_server.py:665] Supported tasks: ['generate']
[0;36m(APIServer pid=821011)[0;0m WARNING 02-04 17:06:24 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:24 [serving.py:177] Warming up chat template processing...
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [hf.py:310] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [serving.py:212] Chat template warmup completed in 1621.9ms
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:06:26 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=821011)[0;0m INFO:     Started server process [821011]
[0;36m(APIServer pid=821011)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=821011)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=821011)[0;0m INFO:     127.0.0.1:47682 - "GET /v1/models HTTP/1.1" 200 OK
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:08:17 [loggers.py:257] Engine 000: Avg prompt throughput: 64.8 tokens/s, Avg generation throughput: 1.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:08:27 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:08:37 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:08:47 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:08:57 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:09:07 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:09:17 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:09:27 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=821011)[0;0m INFO:     127.0.0.1:56444 - "GET /v1/models HTTP/1.1" 200 OK
[0;36m(APIServer pid=821011)[0;0m INFO:     127.0.0.1:57228 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:13:37 [loggers.py:257] Engine 000: Avg prompt throughput: 3.7 tokens/s, Avg generation throughput: 0.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:13:47 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=821011)[0;0m INFO 02-04 17:14:56 [launcher.py:110] Shutting down FastAPI HTTP server.
[0;36m(APIServer pid=821011)[0;0m INFO:     Shutting down
[0;36m(APIServer pid=821011)[0;0m INFO:     Waiting for application shutdown.
[0;36m(APIServer pid=821011)[0;0m INFO:     Application shutdown complete.
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:04 [utils.py:325] 
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:04 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:04 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:04 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   Qwen/Qwen2.5-3B-Instruct-AWQ
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:04 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:04 [utils.py:325] 
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:04 [utils.py:261] non-default args: {'model_tag': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'api_server_count': 1, 'host': '0.0.0.0', 'model': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'max_model_len': 8192, 'quantization': 'awq', 'gpu_memory_utilization': 0.5}
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:05 [model.py:541] Resolved architecture: Qwen2ForCausalLM
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:05 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:05 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:05 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:05 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=827002)[0;0m INFO 02-04 17:15:13 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='Qwen/Qwen2.5-3B-Instruct-AWQ', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen2.5-3B-Instruct-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=827002)[0;0m INFO 02-04 17:15:14 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.0.76:47285 backend=nccl
[0;36m(EngineCore_DP0 pid=827002)[0;0m INFO 02-04 17:15:14 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=827002)[0;0m INFO 02-04 17:15:14 [gpu_model_runner.py:4021] Starting to load model Qwen/Qwen2.5-3B-Instruct-AWQ...
[0;36m(EngineCore_DP0 pid=827002)[0;0m INFO 02-04 17:15:15 [cuda.py:364] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=827002)[0;0m INFO 02-04 17:15:16 [weight_utils.py:567] No model.safetensors.index.json found in remote.
[0;36m(EngineCore_DP0 pid=827002)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=827002)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.09it/s]
[0;36m(EngineCore_DP0 pid=827002)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.09it/s]
[0;36m(EngineCore_DP0 pid=827002)[0;0m 
[0;36m(EngineCore_DP0 pid=827002)[0;0m INFO 02-04 17:15:16 [default_loader.py:291] Loading weights took 0.33 seconds
[0;36m(EngineCore_DP0 pid=827002)[0;0m INFO 02-04 17:15:16 [gpu_model_runner.py:4118] Model loading took 1.95 GiB memory and 1.920956 seconds
[0;36m(EngineCore_DP0 pid=827002)[0;0m INFO 02-04 17:15:24 [backends.py:805] Using cache directory: /home/zodrak/.cache/vllm/torch_compile_cache/4d19c4f626/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=827002)[0;0m INFO 02-04 17:15:24 [backends.py:865] Dynamo bytecode transform time: 7.11 s
[0;36m(EngineCore_DP0 pid=827002)[0;0m INFO 02-04 17:15:32 [backends.py:267] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 0.815 s
[0;36m(EngineCore_DP0 pid=827002)[0;0m INFO 02-04 17:15:32 [monitor.py:34] torch.compile takes 7.93 s in total
[0;36m(EngineCore_DP0 pid=827002)[0;0m INFO 02-04 17:15:32 [gpu_worker.py:356] Available KV cache memory: 2.43 GiB
[0;36m(EngineCore_DP0 pid=827002)[0;0m INFO 02-04 17:15:32 [kv_cache_utils.py:1307] GPU KV cache size: 70,784 tokens
[0;36m(EngineCore_DP0 pid=827002)[0;0m INFO 02-04 17:15:32 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 8.64x
[0;36m(EngineCore_DP0 pid=827002)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|â–         | 1/51 [00:00<00:12,  4.01it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 2/51 [00:00<00:12,  3.99it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 3/51 [00:00<00:11,  4.34it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 4/51 [00:00<00:10,  4.54it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|â–‰         | 5/51 [00:01<00:09,  4.65it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:01<00:09,  4.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|â–ˆâ–Ž        | 7/51 [00:01<00:09,  4.73it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 8/51 [00:01<00:09,  4.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 9/51 [00:01<00:08,  4.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–‰        | 10/51 [00:02<00:08,  4.82it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 11/51 [00:02<00:08,  4.83it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:02<00:08,  4.83it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:02<00:07,  5.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:02<00:07,  5.17it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:03<00:06,  5.31it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:03<00:06,  5.42it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:03<00:05,  5.76it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:03<00:05,  6.07it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:03<00:05,  6.30it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:03<00:04,  6.50it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:04<00:04,  6.62it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:04<00:04,  6.77it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:04<00:04,  6.83it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:04<00:03,  6.87it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:04<00:03,  7.11it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:04<00:03,  7.05it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:04<00:03,  6.99it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:04<00:03,  6.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:05<00:03,  7.23it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:05<00:02,  7.47it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:05<00:02,  7.56it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:05<00:02,  7.58it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:05<00:02,  7.90it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:05<00:02,  8.12it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:05<00:01,  8.28it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:05<00:01,  8.47it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [00:06<00:01,  8.38it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:06<00:01,  8.54it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [00:06<00:01,  8.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [00:06<00:01,  8.82it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [00:06<00:01,  9.02it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [00:06<00:00,  9.29it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:06<00:00,  9.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [00:07<00:00,  9.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [00:07<00:00, 10.01it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [00:07<00:00, 10.08it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:07<00:00,  6.80it/s]
[0;36m(EngineCore_DP0 pid=827002)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   3%|â–Ž         | 1/35 [00:00<00:05,  6.01it/s]Capturing CUDA graphs (decode, FULL):   6%|â–Œ         | 2/35 [00:00<00:05,  6.36it/s]Capturing CUDA graphs (decode, FULL):   9%|â–Š         | 3/35 [00:00<00:04,  6.50it/s]Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:04,  6.60it/s]Capturing CUDA graphs (decode, FULL):  14%|â–ˆâ–        | 5/35 [00:00<00:04,  6.69it/s]Capturing CUDA graphs (decode, FULL):  17%|â–ˆâ–‹        | 6/35 [00:00<00:04,  6.73it/s]Capturing CUDA graphs (decode, FULL):  20%|â–ˆâ–ˆ        | 7/35 [00:01<00:04,  6.71it/s]Capturing CUDA graphs (decode, FULL):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:01<00:03,  6.82it/s]Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:01<00:03,  7.11it/s]Capturing CUDA graphs (decode, FULL):  29%|â–ˆâ–ˆâ–Š       | 10/35 [00:01<00:03,  7.02it/s]Capturing CUDA graphs (decode, FULL):  31%|â–ˆâ–ˆâ–ˆâ–      | 11/35 [00:01<00:03,  6.97it/s]Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:01<00:03,  6.93it/s]Capturing CUDA graphs (decode, FULL):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [00:01<00:03,  7.13it/s]Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:02<00:02,  7.33it/s]Capturing CUDA graphs (decode, FULL):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15/35 [00:02<00:02,  7.47it/s]Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:02<00:02,  7.58it/s]Capturing CUDA graphs (decode, FULL):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [00:02<00:02,  7.91it/s]Capturing CUDA graphs (decode, FULL):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:02<00:02,  8.14it/s]Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:02<00:01,  8.35it/s]Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:02<00:01,  8.49it/s]Capturing CUDA graphs (decode, FULL):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:02<00:01,  8.56it/s]Capturing CUDA graphs (decode, FULL):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 22/35 [00:02<00:01,  8.81it/s]Capturing CUDA graphs (decode, FULL):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 23/35 [00:03<00:01,  9.06it/s]Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:03<00:01,  9.15it/s]Capturing CUDA graphs (decode, FULL):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26/35 [00:03<00:00,  9.76it/s]Capturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:03<00:00, 10.21it/s]Capturing CUDA graphs (decode, FULL):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 30/35 [00:03<00:00, 10.42it/s]Capturing CUDA graphs (decode, FULL):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:03<00:00, 10.55it/s]Capturing CUDA graphs (decode, FULL):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:04<00:00, 10.69it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  8.38it/s]
[0;36m(EngineCore_DP0 pid=827002)[0;0m INFO 02-04 17:15:44 [gpu_model_runner.py:5051] Graph capturing finished in 12 secs, took 0.72 GiB
[0;36m(EngineCore_DP0 pid=827002)[0;0m INFO 02-04 17:15:44 [core.py:272] init engine (profile, create kv cache, warmup model) took 28.03 seconds
[0;36m(EngineCore_DP0 pid=827002)[0;0m INFO 02-04 17:15:46 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:46 [api_server.py:665] Supported tasks: ['generate']
[0;36m(APIServer pid=826106)[0;0m WARNING 02-04 17:15:46 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:46 [serving.py:177] Warming up chat template processing...
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [hf.py:310] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [serving.py:212] Chat template warmup completed in 1533.2ms
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:15:48 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=826106)[0;0m INFO:     Started server process [826106]
[0;36m(APIServer pid=826106)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=826106)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:20:38 [loggers.py:257] Engine 000: Avg prompt throughput: 64.8 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:20:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:20:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:21:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:21:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:21:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:21:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:21:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:21:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:22:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:22:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:22:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:22:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:22:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:22:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:57874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:23:08 [loggers.py:257] Engine 000: Avg prompt throughput: 209.1 tokens/s, Avg generation throughput: 13.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:23:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:23:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:23:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:23:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:23:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:24:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:24:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:24:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:24:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:24:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:24:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:25:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:25:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:44834 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:25:28 [loggers.py:257] Engine 000: Avg prompt throughput: 66.0 tokens/s, Avg generation throughput: 13.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:25:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:25:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:25:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:26:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:26:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:26:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:26:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:26:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:26:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:48832 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:27:08 [loggers.py:257] Engine 000: Avg prompt throughput: 58.8 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.8%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:27:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.8%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:27:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.8%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:27:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.8%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:33186 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:27:48 [loggers.py:257] Engine 000: Avg prompt throughput: 105.4 tokens/s, Avg generation throughput: 14.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 1.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:27:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 1.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:28:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 1.9%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:57026 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:28:18 [loggers.py:257] Engine 000: Avg prompt throughput: 389.1 tokens/s, Avg generation throughput: 12.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.5%, Prefix cache hit rate: 1.3%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:36736 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:28:28 [loggers.py:257] Engine 000: Avg prompt throughput: 76.2 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 7.8%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:28:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 7.8%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:28:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 7.8%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:28:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 7.8%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:29:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 7.8%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:29:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 7.8%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:48700 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:29:28 [loggers.py:257] Engine 000: Avg prompt throughput: 58.8 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 7.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:29:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 7.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:29:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 7.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:29:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 7.9%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:33822 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:30:08 [loggers.py:257] Engine 000: Avg prompt throughput: 105.4 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 16.4%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:30:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 16.4%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:30:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 16.4%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:30:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 16.4%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:56846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:30:48 [loggers.py:257] Engine 000: Avg prompt throughput: 389.1 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.7%, Prefix cache hit rate: 37.7%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:60196 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:30:58 [loggers.py:257] Engine 000: Avg prompt throughput: 76.2 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 40.6%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:31:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 40.6%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:31:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 40.6%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:31:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 40.6%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:31:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 40.6%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:31:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 40.6%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:31:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 40.6%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:32:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 40.6%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:32:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 40.6%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:60204 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:32:28 [loggers.py:257] Engine 000: Avg prompt throughput: 58.8 tokens/s, Avg generation throughput: 13.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 39.6%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:32:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 39.6%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:32:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 39.6%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:33126 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:32:58 [loggers.py:257] Engine 000: Avg prompt throughput: 105.4 tokens/s, Avg generation throughput: 13.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 43.1%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:33:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 43.1%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:33:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 43.1%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:33:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 43.1%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:53302 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:33:38 [loggers.py:257] Engine 000: Avg prompt throughput: 389.1 tokens/s, Avg generation throughput: 13.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.5%, Prefix cache hit rate: 53.4%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:39172 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:33:48 [loggers.py:257] Engine 000: Avg prompt throughput: 76.2 tokens/s, Avg generation throughput: 13.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 54.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:33:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 54.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:34:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 54.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:34:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 54.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:34:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 54.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:34:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 54.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:34:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 54.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:34:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 54.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:35:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 54.9%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:42572 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:35:18 [loggers.py:257] Engine 000: Avg prompt throughput: 58.8 tokens/s, Avg generation throughput: 14.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:35:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:35:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:35:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:46876 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:35:58 [loggers.py:257] Engine 000: Avg prompt throughput: 105.4 tokens/s, Avg generation throughput: 13.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 57.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:36:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 57.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:36:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 57.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:36:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 57.9%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:35444 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:36:38 [loggers.py:257] Engine 000: Avg prompt throughput: 389.1 tokens/s, Avg generation throughput: 13.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.6%, Prefix cache hit rate: 63.8%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:33990 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:36:48 [loggers.py:257] Engine 000: Avg prompt throughput: 389.1 tokens/s, Avg generation throughput: 13.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.7%, Prefix cache hit rate: 68.2%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:33992 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:36:58 [loggers.py:257] Engine 000: Avg prompt throughput: 76.2 tokens/s, Avg generation throughput: 13.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 68.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:37:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 68.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:37:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 68.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:37:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 68.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:37:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 68.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:37:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 68.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:37:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 68.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:38:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 68.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:38:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.9%, Prefix cache hit rate: 68.9%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:43934 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:38:28 [loggers.py:257] Engine 000: Avg prompt throughput: 58.8 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 69.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:38:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 69.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:38:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 69.5%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:57110 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:38:58 [loggers.py:257] Engine 000: Avg prompt throughput: 105.4 tokens/s, Avg generation throughput: 14.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 70.4%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:39:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 70.4%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:39:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 70.4%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:39:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 70.4%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:46354 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:39:38 [loggers.py:257] Engine 000: Avg prompt throughput: 389.1 tokens/s, Avg generation throughput: 13.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.5%, Prefix cache hit rate: 73.4%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:38150 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:39:48 [loggers.py:257] Engine 000: Avg prompt throughput: 76.2 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 73.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:39:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 73.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:40:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 73.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:40:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 73.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:40:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 73.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:40:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 73.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:40:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 73.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:40:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 73.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:41:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 73.9%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:44962 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:41:18 [loggers.py:257] Engine 000: Avg prompt throughput: 58.8 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 74.2%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:41:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 74.2%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:41:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 74.2%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:41:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 74.2%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:52884 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:41:58 [loggers.py:257] Engine 000: Avg prompt throughput: 105.4 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 74.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:42:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 74.9%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:42:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 74.9%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:49264 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:42:28 [loggers.py:257] Engine 000: Avg prompt throughput: 389.1 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.5%, Prefix cache hit rate: 77.1%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:36964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:42:38 [loggers.py:257] Engine 000: Avg prompt throughput: 76.2 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 77.4%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:42:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 77.4%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:42:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 77.4%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:43:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 77.4%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:43:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 77.4%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:43:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 77.4%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:43:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 77.4%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:43:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 77.4%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:43:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 77.4%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:56190 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:44:08 [loggers.py:257] Engine 000: Avg prompt throughput: 58.8 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 77.7%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:44:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 77.7%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:44:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 77.7%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:44:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 77.7%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:54032 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:44:48 [loggers.py:257] Engine 000: Avg prompt throughput: 105.4 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 78.2%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:44:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 78.2%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:45:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 78.2%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:45:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 78.2%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:59678 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:46050 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:45:28 [loggers.py:257] Engine 000: Avg prompt throughput: 778.1 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.5%, Prefix cache hit rate: 81.3%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:53240 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:45:38 [loggers.py:257] Engine 000: Avg prompt throughput: 76.2 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 81.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:45:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 81.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:45:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 81.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:46:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 81.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:46:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 81.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:46:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 81.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:46:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 81.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:46:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 81.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:46:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 81.5%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:48260 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:47:08 [loggers.py:257] Engine 000: Avg prompt throughput: 58.8 tokens/s, Avg generation throughput: 13.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 81.7%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:47:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 81.7%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:47:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 81.7%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:47:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 81.7%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:38892 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:47:48 [loggers.py:257] Engine 000: Avg prompt throughput: 105.4 tokens/s, Avg generation throughput: 14.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 82.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:47:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 82.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:48:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 82.0%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:48:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 82.0%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:40604 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:43388 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:48:28 [loggers.py:257] Engine 000: Avg prompt throughput: 465.3 tokens/s, Avg generation throughput: 13.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 83.4%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:48:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 83.4%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:48:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 83.4%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:48:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 83.4%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:49:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 83.4%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:49:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 83.4%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:49:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 83.4%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:49:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 83.4%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:49:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 83.4%
[0;36m(APIServer pid=826106)[0;0m INFO:     127.0.0.1:55232 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:49:58 [loggers.py:257] Engine 000: Avg prompt throughput: 58.8 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 83.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:50:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 83.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:50:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 83.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:50:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 83.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:50:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 83.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:50:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 83.5%
[0;36m(APIServer pid=826106)[0;0m INFO 02-04 17:51:19 [launcher.py:110] Shutting down FastAPI HTTP server.
[0;36m(APIServer pid=826106)[0;0m INFO:     Shutting down
[0;36m(APIServer pid=826106)[0;0m INFO:     Waiting for application shutdown.
[0;36m(APIServer pid=826106)[0;0m INFO:     Application shutdown complete.
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:51:27 [utils.py:325] 
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:51:27 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:51:27 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:51:27 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   Qwen/Qwen2.5-3B-Instruct-AWQ
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:51:27 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:51:27 [utils.py:325] 
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:51:27 [utils.py:261] non-default args: {'model_tag': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'api_server_count': 1, 'host': '0.0.0.0', 'model': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'max_model_len': 8192, 'quantization': 'awq', 'gpu_memory_utilization': 0.5}
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:51:27 [model.py:541] Resolved architecture: Qwen2ForCausalLM
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:51:27 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:51:28 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:51:28 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:51:28 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=833824)[0;0m INFO 02-04 17:51:36 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='Qwen/Qwen2.5-3B-Instruct-AWQ', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen2.5-3B-Instruct-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=833824)[0;0m INFO 02-04 17:51:36 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.0.76:51407 backend=nccl
[0;36m(EngineCore_DP0 pid=833824)[0;0m INFO 02-04 17:51:36 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=833824)[0;0m INFO 02-04 17:51:37 [gpu_model_runner.py:4021] Starting to load model Qwen/Qwen2.5-3B-Instruct-AWQ...
[0;36m(EngineCore_DP0 pid=833824)[0;0m INFO 02-04 17:51:37 [cuda.py:364] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=833824)[0;0m INFO 02-04 17:51:38 [weight_utils.py:567] No model.safetensors.index.json found in remote.
[0;36m(EngineCore_DP0 pid=833824)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=833824)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.09it/s]
[0;36m(EngineCore_DP0 pid=833824)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.08it/s]
[0;36m(EngineCore_DP0 pid=833824)[0;0m 
[0;36m(EngineCore_DP0 pid=833824)[0;0m INFO 02-04 17:51:39 [default_loader.py:291] Loading weights took 0.33 seconds
[0;36m(EngineCore_DP0 pid=833824)[0;0m INFO 02-04 17:51:39 [gpu_model_runner.py:4118] Model loading took 1.95 GiB memory and 1.968660 seconds
[0;36m(EngineCore_DP0 pid=833824)[0;0m INFO 02-04 17:51:47 [backends.py:805] Using cache directory: /home/zodrak/.cache/vllm/torch_compile_cache/4d19c4f626/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=833824)[0;0m INFO 02-04 17:51:47 [backends.py:865] Dynamo bytecode transform time: 7.44 s
[0;36m(EngineCore_DP0 pid=833824)[0;0m INFO 02-04 17:51:55 [backends.py:267] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 0.811 s
[0;36m(EngineCore_DP0 pid=833824)[0;0m INFO 02-04 17:51:55 [monitor.py:34] torch.compile takes 8.25 s in total
[0;36m(EngineCore_DP0 pid=833824)[0;0m INFO 02-04 17:51:56 [gpu_worker.py:356] Available KV cache memory: 2.43 GiB
[0;36m(EngineCore_DP0 pid=833824)[0;0m INFO 02-04 17:51:56 [kv_cache_utils.py:1307] GPU KV cache size: 70,784 tokens
[0;36m(EngineCore_DP0 pid=833824)[0;0m INFO 02-04 17:51:56 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 8.64x
[0;36m(EngineCore_DP0 pid=833824)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|â–         | 1/51 [00:00<00:12,  4.05it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 2/51 [00:00<00:12,  4.07it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 3/51 [00:00<00:11,  4.33it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 4/51 [00:00<00:10,  4.54it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|â–‰         | 5/51 [00:01<00:09,  4.67it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:01<00:09,  4.77it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|â–ˆâ–Ž        | 7/51 [00:01<00:09,  4.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 8/51 [00:01<00:08,  4.85it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 9/51 [00:01<00:08,  4.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–‰        | 10/51 [00:02<00:08,  4.92it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 11/51 [00:02<00:08,  4.94it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:02<00:07,  4.90it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:02<00:07,  5.11it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:02<00:07,  5.28it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:03<00:06,  5.40it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:03<00:06,  5.50it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:03<00:05,  5.87it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:03<00:05,  6.15it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:03<00:05,  6.39it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:03<00:04,  6.57it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:03<00:04,  6.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:04<00:04,  6.83it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:04<00:04,  6.91it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:04<00:03,  6.98it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:04<00:03,  7.20it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:04<00:03,  7.06it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:04<00:03,  7.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:04<00:03,  6.90it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:05<00:03,  7.18it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:05<00:02,  7.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:05<00:02,  7.63it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:05<00:02,  7.68it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:05<00:02,  7.96it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:05<00:02,  8.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:05<00:01,  8.40it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:05<00:01,  8.57it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [00:06<00:01,  8.43it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:06<00:01,  8.71it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [00:06<00:01,  8.99it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [00:06<00:01,  9.05it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [00:06<00:00,  9.63it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:06<00:00, 10.00it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [00:06<00:00, 10.17it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [00:07<00:00, 10.34it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [00:07<00:00, 10.41it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:07<00:00,  6.92it/s]
[0;36m(EngineCore_DP0 pid=833824)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   3%|â–Ž         | 1/35 [00:00<00:05,  6.05it/s]Capturing CUDA graphs (decode, FULL):   6%|â–Œ         | 2/35 [00:00<00:05,  6.50it/s]Capturing CUDA graphs (decode, FULL):   9%|â–Š         | 3/35 [00:00<00:04,  6.58it/s]Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:04,  6.64it/s]Capturing CUDA graphs (decode, FULL):  14%|â–ˆâ–        | 5/35 [00:00<00:04,  6.65it/s]Capturing CUDA graphs (decode, FULL):  17%|â–ˆâ–‹        | 6/35 [00:00<00:04,  6.77it/s]Capturing CUDA graphs (decode, FULL):  20%|â–ˆâ–ˆ        | 7/35 [00:01<00:04,  6.83it/s]Capturing CUDA graphs (decode, FULL):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:01<00:03,  6.85it/s]Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:01<00:03,  7.08it/s]Capturing CUDA graphs (decode, FULL):  29%|â–ˆâ–ˆâ–Š       | 10/35 [00:01<00:03,  7.02it/s]Capturing CUDA graphs (decode, FULL):  31%|â–ˆâ–ˆâ–ˆâ–      | 11/35 [00:01<00:03,  6.99it/s]Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:01<00:03,  6.95it/s]Capturing CUDA graphs (decode, FULL):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [00:01<00:03,  7.26it/s]Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:02<00:02,  7.49it/s]Capturing CUDA graphs (decode, FULL):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15/35 [00:02<00:02,  7.61it/s]Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:02<00:02,  7.64it/s]Capturing CUDA graphs (decode, FULL):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [00:02<00:02,  7.96it/s]Capturing CUDA graphs (decode, FULL):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:02<00:02,  8.17it/s]Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:02<00:01,  8.34it/s]Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:02<00:01,  8.48it/s]Capturing CUDA graphs (decode, FULL):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:02<00:01,  8.49it/s]Capturing CUDA graphs (decode, FULL):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 22/35 [00:02<00:01,  8.75it/s]Capturing CUDA graphs (decode, FULL):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 23/35 [00:03<00:01,  8.99it/s]Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:03<00:01,  9.18it/s]Capturing CUDA graphs (decode, FULL):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26/35 [00:03<00:00,  9.90it/s]Capturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:03<00:00, 10.33it/s]Capturing CUDA graphs (decode, FULL):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 30/35 [00:03<00:00, 10.58it/s]Capturing CUDA graphs (decode, FULL):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:03<00:00, 10.70it/s]Capturing CUDA graphs (decode, FULL):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:04<00:00, 10.88it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  8.45it/s]
[0;36m(EngineCore_DP0 pid=833824)[0;0m INFO 02-04 17:52:08 [gpu_model_runner.py:5051] Graph capturing finished in 12 secs, took 0.72 GiB
[0;36m(EngineCore_DP0 pid=833824)[0;0m INFO 02-04 17:52:08 [core.py:272] init engine (profile, create kv cache, warmup model) took 28.65 seconds
[0;36m(EngineCore_DP0 pid=833824)[0;0m INFO 02-04 17:52:09 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:09 [api_server.py:665] Supported tasks: ['generate']
[0;36m(APIServer pid=832925)[0;0m WARNING 02-04 17:52:10 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:10 [serving.py:177] Warming up chat template processing...
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:11 [hf.py:310] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:11 [serving.py:212] Chat template warmup completed in 1547.5ms
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:52:12 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=832925)[0;0m INFO:     Started server process [832925]
[0;36m(APIServer pid=832925)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=832925)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:54:42 [loggers.py:257] Engine 000: Avg prompt throughput: 339.5 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=832925)[0;0m INFO:     127.0.0.1:55504 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:54:52 [loggers.py:257] Engine 000: Avg prompt throughput: 369.0 tokens/s, Avg generation throughput: 13.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.4%, Prefix cache hit rate: 0.2%
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:55:02 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.6%, Prefix cache hit rate: 0.2%
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:55:12 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.8%, Prefix cache hit rate: 0.2%
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:55:22 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.0%, Prefix cache hit rate: 0.2%
[0;36m(APIServer pid=832925)[0;0m INFO:     127.0.0.1:39820 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:55:32 [loggers.py:257] Engine 000: Avg prompt throughput: 339.3 tokens/s, Avg generation throughput: 13.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.9%, Prefix cache hit rate: 0.3%
[0;36m(APIServer pid=832925)[0;0m INFO:     127.0.0.1:55222 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:55:42 [loggers.py:257] Engine 000: Avg prompt throughput: 342.6 tokens/s, Avg generation throughput: 13.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.0%, Prefix cache hit rate: 0.3%
[0;36m(APIServer pid=832925)[0;0m INFO:     127.0.0.1:50808 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:55:52 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.3%
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:56:02 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.3%
[0;36m(APIServer pid=832925)[0;0m INFO 02-04 17:59:17 [launcher.py:110] Shutting down FastAPI HTTP server.
[0;36m(APIServer pid=832925)[0;0m INFO:     Shutting down
[0;36m(APIServer pid=832925)[0;0m INFO:     Waiting for application shutdown.
[0;36m(APIServer pid=832925)[0;0m INFO:     Application shutdown complete.
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:50:45 [utils.py:325] 
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:50:45 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:50:45 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:50:45 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   Qwen/Qwen2.5-3B-Instruct-AWQ
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:50:45 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:50:45 [utils.py:325] 
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:50:45 [utils.py:261] non-default args: {'model_tag': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'api_server_count': 1, 'host': '0.0.0.0', 'model': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'max_model_len': 8192, 'quantization': 'awq', 'gpu_memory_utilization': 0.5}
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:50:46 [model.py:541] Resolved architecture: Qwen2ForCausalLM
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:50:46 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:50:47 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:50:47 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:50:47 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=9238)[0;0m INFO 02-07 15:50:56 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='Qwen/Qwen2.5-3B-Instruct-AWQ', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen2.5-3B-Instruct-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=9238)[0;0m INFO 02-07 15:50:56 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.0.76:48537 backend=nccl
[0;36m(EngineCore_DP0 pid=9238)[0;0m INFO 02-07 15:50:56 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=9238)[0;0m INFO 02-07 15:50:57 [gpu_model_runner.py:4021] Starting to load model Qwen/Qwen2.5-3B-Instruct-AWQ...
[0;36m(EngineCore_DP0 pid=9238)[0;0m INFO 02-07 15:50:58 [cuda.py:364] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=9238)[0;0m INFO 02-07 15:50:59 [weight_utils.py:567] No model.safetensors.index.json found in remote.
[0;36m(EngineCore_DP0 pid=9238)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=9238)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.29s/it]
[0;36m(EngineCore_DP0 pid=9238)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.29s/it]
[0;36m(EngineCore_DP0 pid=9238)[0;0m 
[0;36m(EngineCore_DP0 pid=9238)[0;0m INFO 02-07 15:51:01 [default_loader.py:291] Loading weights took 1.30 seconds
[0;36m(EngineCore_DP0 pid=9238)[0;0m INFO 02-07 15:51:01 [gpu_model_runner.py:4118] Model loading took 1.95 GiB memory and 3.431613 seconds
[0;36m(EngineCore_DP0 pid=9238)[0;0m INFO 02-07 15:51:09 [backends.py:805] Using cache directory: /home/zodrak/.cache/vllm/torch_compile_cache/4d19c4f626/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=9238)[0;0m INFO 02-07 15:51:09 [backends.py:865] Dynamo bytecode transform time: 7.46 s
[0;36m(EngineCore_DP0 pid=9238)[0;0m INFO 02-07 15:51:17 [backends.py:267] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 0.896 s
[0;36m(EngineCore_DP0 pid=9238)[0;0m INFO 02-07 15:51:17 [monitor.py:34] torch.compile takes 8.35 s in total
[0;36m(EngineCore_DP0 pid=9238)[0;0m INFO 02-07 15:51:18 [gpu_worker.py:356] Available KV cache memory: 2.42 GiB
[0;36m(EngineCore_DP0 pid=9238)[0;0m INFO 02-07 15:51:18 [kv_cache_utils.py:1307] GPU KV cache size: 70,576 tokens
[0;36m(EngineCore_DP0 pid=9238)[0;0m INFO 02-07 15:51:18 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 8.62x
[0;36m(EngineCore_DP0 pid=9238)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|â–         | 1/51 [00:00<00:12,  3.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 2/51 [00:00<00:11,  4.11it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 3/51 [00:00<00:10,  4.51it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 4/51 [00:00<00:09,  4.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|â–‰         | 5/51 [00:01<00:09,  4.87it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:01<00:09,  4.97it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|â–ˆâ–Ž        | 7/51 [00:01<00:08,  5.06it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 8/51 [00:01<00:08,  5.07it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 9/51 [00:01<00:08,  5.16it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–‰        | 10/51 [00:02<00:07,  5.18it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 11/51 [00:02<00:07,  5.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:02<00:07,  5.24it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:02<00:06,  5.46it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:02<00:06,  5.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:02<00:06,  5.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:03<00:06,  5.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:03<00:05,  6.18it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:03<00:05,  6.35it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:03<00:04,  6.52it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:03<00:04,  6.63it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:03<00:04,  6.76it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:03<00:04,  6.92it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:04<00:03,  7.06it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:04<00:03,  7.17it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:04<00:03,  7.43it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:04<00:03,  7.31it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:04<00:03,  7.28it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:04<00:03,  7.24it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:04<00:02,  7.51it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:04<00:02,  7.74it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:05<00:02,  7.84it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:05<00:02,  7.91it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:05<00:02,  8.16it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:05<00:02,  8.32it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:05<00:01,  8.45it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:05<00:01,  8.63it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [00:05<00:01,  8.33it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:05<00:01,  8.64it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [00:06<00:01,  8.97it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [00:06<00:01,  9.22it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [00:06<00:00,  9.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:06<00:00,  9.90it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [00:06<00:00, 10.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [00:06<00:00, 10.22it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [00:07<00:00, 10.43it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:07<00:00,  7.11it/s]
[0;36m(EngineCore_DP0 pid=9238)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   3%|â–Ž         | 1/35 [00:00<00:05,  6.20it/s]Capturing CUDA graphs (decode, FULL):   6%|â–Œ         | 2/35 [00:00<00:04,  6.61it/s]Capturing CUDA graphs (decode, FULL):   9%|â–Š         | 3/35 [00:00<00:04,  6.83it/s]Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:04,  6.94it/s]Capturing CUDA graphs (decode, FULL):  14%|â–ˆâ–        | 5/35 [00:00<00:04,  7.00it/s]Capturing CUDA graphs (decode, FULL):  17%|â–ˆâ–‹        | 6/35 [00:00<00:04,  7.08it/s]Capturing CUDA graphs (decode, FULL):  20%|â–ˆâ–ˆ        | 7/35 [00:01<00:03,  7.10it/s]Capturing CUDA graphs (decode, FULL):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:01<00:03,  7.17it/s]Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:01<00:03,  7.30it/s]Capturing CUDA graphs (decode, FULL):  29%|â–ˆâ–ˆâ–Š       | 10/35 [00:01<00:03,  7.15it/s]Capturing CUDA graphs (decode, FULL):  31%|â–ˆâ–ˆâ–ˆâ–      | 11/35 [00:01<00:03,  7.10it/s]Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:01<00:03,  7.10it/s]Capturing CUDA graphs (decode, FULL):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [00:01<00:02,  7.42it/s]Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:01<00:02,  7.67it/s]Capturing CUDA graphs (decode, FULL):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15/35 [00:02<00:02,  7.81it/s]Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:02<00:02,  7.89it/s]Capturing CUDA graphs (decode, FULL):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [00:02<00:02,  8.23it/s]Capturing CUDA graphs (decode, FULL):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:02<00:02,  8.49it/s]Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:02<00:01,  8.64it/s]Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:02<00:01,  8.83it/s]Capturing CUDA graphs (decode, FULL):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:02<00:01,  8.94it/s]Capturing CUDA graphs (decode, FULL):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 22/35 [00:02<00:01,  9.21it/s]Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:03<00:01,  9.66it/s]Capturing CUDA graphs (decode, FULL):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26/35 [00:03<00:00, 10.15it/s]Capturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:03<00:00, 10.54it/s]Capturing CUDA graphs (decode, FULL):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 30/35 [00:03<00:00, 10.78it/s]Capturing CUDA graphs (decode, FULL):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:03<00:00, 10.82it/s]Capturing CUDA graphs (decode, FULL):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:03<00:00, 10.98it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  8.72it/s]
[0;36m(EngineCore_DP0 pid=9238)[0;0m INFO 02-07 15:51:30 [gpu_model_runner.py:5051] Graph capturing finished in 12 secs, took 0.82 GiB
[0;36m(EngineCore_DP0 pid=9238)[0;0m INFO 02-07 15:51:30 [core.py:272] init engine (profile, create kv cache, warmup model) took 28.67 seconds
[0;36m(EngineCore_DP0 pid=9238)[0;0m INFO 02-07 15:51:31 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:31 [api_server.py:665] Supported tasks: ['generate']
[0;36m(APIServer pid=7765)[0;0m WARNING 02-07 15:51:31 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:31 [serving.py:177] Warming up chat template processing...
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [hf.py:310] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [serving.py:212] Chat template warmup completed in 26411.0ms
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:51:58 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=7765)[0;0m INFO:     Started server process [7765]
[0;36m(APIServer pid=7765)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=7765)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:52:58 [loggers.py:257] Engine 000: Avg prompt throughput: 16.5 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:53:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:37566 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:53:18 [loggers.py:257] Engine 000: Avg prompt throughput: 63.5 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 2.0%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:53:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 2.0%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:53:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 2.0%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:53:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 2.0%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:53:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 2.0%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:54:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 2.0%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:54:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 2.0%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:54:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 2.0%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:54:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 2.0%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:46226 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:54:48 [loggers.py:257] Engine 000: Avg prompt throughput: 57.5 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 2.3%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:54:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 2.3%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:55:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 2.3%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:55:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 2.3%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:38168 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:55:28 [loggers.py:257] Engine 000: Avg prompt throughput: 102.8 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 4.0%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:55:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 4.0%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:55:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 4.0%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:44914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:55:58 [loggers.py:257] Engine 000: Avg prompt throughput: 338.2 tokens/s, Avg generation throughput: 13.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.8%, Prefix cache hit rate: 1.9%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:41478 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:56:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 10.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.9%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:56:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.9%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:57:38 [loggers.py:257] Engine 000: Avg prompt throughput: 367.7 tokens/s, Avg generation throughput: 11.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.4%, Prefix cache hit rate: 1.4%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:50788 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:57:48 [loggers.py:257] Engine 000: Avg prompt throughput: 338.0 tokens/s, Avg generation throughput: 13.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.9%, Prefix cache hit rate: 1.1%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:44330 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:57:58 [loggers.py:257] Engine 000: Avg prompt throughput: 341.3 tokens/s, Avg generation throughput: 13.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.9%, Prefix cache hit rate: 1.0%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:44344 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:58:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.0%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 15:58:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.0%
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:03 [serving.py:323] Error in preprocessing prompt inputs
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:03 [serving.py:323] Traceback (most recent call last):
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:03 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:03 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:03 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:03 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:03 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:03 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:03 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:03 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:03 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:03 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:03 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:03 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:03 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:03 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:03 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:03 [serving.py:323]     raise VLLMValidationError(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:03 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 2048. This model's maximum context length is 8192 tokens and your request has 6910 input tokens (2048 > 8192 - 6910). (parameter=max_tokens, value=2048)
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:36924 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:20:09 [loggers.py:257] Engine 000: Avg prompt throughput: 691.0 tokens/s, Avg generation throughput: 5.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.9%, Prefix cache hit rate: 0.8%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:20:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.1%, Prefix cache hit rate: 0.8%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:36936 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:20:29 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.8%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:20:39 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.8%
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:49 [serving.py:323] Error in preprocessing prompt inputs
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:49 [serving.py:323] Traceback (most recent call last):
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:49 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:49 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:49 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:49 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:49 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:49 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:49 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:49 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:49 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:49 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:49 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:49 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:49 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:49 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:49 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:49 [serving.py:323]     raise VLLMValidationError(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:20:49 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 2048. This model's maximum context length is 8192 tokens and your request has 6910 input tokens (2048 > 8192 - 6910). (parameter=max_tokens, value=2048)
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:46340 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:20:59 [loggers.py:257] Engine 000: Avg prompt throughput: 691.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.0%, Prefix cache hit rate: 23.5%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:46346 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:21:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 23.5%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:21:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 23.5%
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:19 [serving.py:323] Error in preprocessing prompt inputs
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:19 [serving.py:323] Traceback (most recent call last):
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:19 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:19 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:19 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:19 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:19 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:19 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:19 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:19 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:19 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:19 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:19 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:19 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:19 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:19 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:19 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:19 [serving.py:323]     raise VLLMValidationError(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:19 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 2048. This model's maximum context length is 8192 tokens and your request has 6910 input tokens (2048 > 8192 - 6910). (parameter=max_tokens, value=2048)
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:40630 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:21:29 [loggers.py:257] Engine 000: Avg prompt throughput: 691.0 tokens/s, Avg generation throughput: 13.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.0%, Prefix cache hit rate: 37.8%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:40638 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:38 [serving.py:323] Error in preprocessing prompt inputs
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:38 [serving.py:323] Traceback (most recent call last):
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:38 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:38 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:38 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:38 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:38 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:38 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:38 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:38 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:38 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:38 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:38 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:38 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:38 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:38 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:38 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:38 [serving.py:323]     raise VLLMValidationError(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:38 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 2048. This model's maximum context length is 8192 tokens and your request has 7062 input tokens (2048 > 8192 - 7062). (parameter=max_tokens, value=2048)
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:53574 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:21:39 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 37.8%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:21:49 [loggers.py:257] Engine 000: Avg prompt throughput: 706.1 tokens/s, Avg generation throughput: 11.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.2%, Prefix cache hit rate: 31.8%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:53588 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:56 [serving.py:323] Error in preprocessing prompt inputs
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:56 [serving.py:323] Traceback (most recent call last):
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:56 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:56 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:56 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:56 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:56 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:56 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:56 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:56 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:56 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:56 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:56 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:56 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:56 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:56 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:56 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:56 [serving.py:323]     raise VLLMValidationError(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:21:56 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 2048. This model's maximum context length is 8192 tokens and your request has 7062 input tokens (2048 > 8192 - 7062). (parameter=max_tokens, value=2048)
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:51094 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:21:59 [loggers.py:257] Engine 000: Avg prompt throughput: 706.2 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.1%, Prefix cache hit rate: 41.2%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:22:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.3%, Prefix cache hit rate: 41.2%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:51106 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:22:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 41.2%
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:22:25 [serving.py:323] Error in preprocessing prompt inputs
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:22:25 [serving.py:323] Traceback (most recent call last):
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:22:25 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:22:25 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:22:25 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:22:25 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:22:25 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:22:25 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:22:25 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:22:25 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:22:25 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:22:25 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:22:25 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:22:25 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:22:25 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:22:25 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:22:25 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:22:25 [serving.py:323]     raise VLLMValidationError(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 17:22:25 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 2048. This model's maximum context length is 8192 tokens and your request has 6910 input tokens (2048 > 8192 - 6910). (parameter=max_tokens, value=2048)
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:39672 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:22:29 [loggers.py:257] Engine 000: Avg prompt throughput: 691.0 tokens/s, Avg generation throughput: 4.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.9%, Prefix cache hit rate: 48.2%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:22:39 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.1%, Prefix cache hit rate: 48.2%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:39684 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:22:49 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 48.2%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:22:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 48.2%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:27:39 [loggers.py:257] Engine 000: Avg prompt throughput: 20.6 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 48.0%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:57680 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:43194 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:27:49 [loggers.py:257] Engine 000: Avg prompt throughput: 71.5 tokens/s, Avg generation throughput: 14.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 47.5%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:27:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 47.5%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:43200 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:28:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 47.5%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:28:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 47.5%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:51:19 [loggers.py:257] Engine 000: Avg prompt throughput: 20.6 tokens/s, Avg generation throughput: 5.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 47.7%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:54852 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:51:29 [loggers.py:257] Engine 000: Avg prompt throughput: 62.9 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 48.2%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:51:39 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 48.2%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:54856 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:51:49 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 48.2%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 17:51:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 48.2%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:09:59 [loggers.py:257] Engine 000: Avg prompt throughput: 23.3 tokens/s, Avg generation throughput: 10.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 48.1%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:58504 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:10:09 [loggers.py:257] Engine 000: Avg prompt throughput: 62.9 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 48.6%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:10:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 48.6%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:41406 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:10:29 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 48.6%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:10:39 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 48.6%
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 18:16:46 [serving.py:323] Error in preprocessing prompt inputs
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 18:16:46 [serving.py:323] Traceback (most recent call last):
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 18:16:46 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 18:16:46 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 18:16:46 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 18:16:46 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 18:16:46 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 18:16:46 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 18:16:46 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 18:16:46 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 18:16:46 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 18:16:46 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 18:16:46 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 18:16:46 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 18:16:46 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 18:16:46 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 18:16:46 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 18:16:46 [serving.py:323]     raise VLLMValidationError(
[0;36m(APIServer pid=7765)[0;0m ERROR 02-07 18:16:46 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 2048. This model's maximum context length is 8192 tokens and your request has 6905 input tokens (2048 > 8192 - 6905). (parameter=max_tokens, value=2048)
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:47976 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:16:49 [loggers.py:257] Engine 000: Avg prompt throughput: 690.4 tokens/s, Avg generation throughput: 2.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.8%, Prefix cache hit rate: 48.7%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:16:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.0%, Prefix cache hit rate: 48.7%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:47978 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:17:09 [loggers.py:257] Engine 000: Avg prompt throughput: 23.3 tokens/s, Avg generation throughput: 9.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 48.9%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:39946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:17:19 [loggers.py:257] Engine 000: Avg prompt throughput: 62.9 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 49.3%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:17:29 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 49.3%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:60156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:17:39 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 49.3%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:17:49 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 49.3%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:23:39 [loggers.py:257] Engine 000: Avg prompt throughput: 23.3 tokens/s, Avg generation throughput: 8.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 49.5%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:50972 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:23:49 [loggers.py:257] Engine 000: Avg prompt throughput: 62.9 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 49.9%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:23:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 49.9%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:50986 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:24:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 49.9%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:24:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 49.9%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:25:49 [loggers.py:257] Engine 000: Avg prompt throughput: 23.3 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 50.1%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:33708 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:25:59 [loggers.py:257] Engine 000: Avg prompt throughput: 62.9 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 50.5%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:26:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 50.5%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:58030 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:26:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 50.5%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:26:29 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 50.5%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:54864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:26:59 [loggers.py:257] Engine 000: Avg prompt throughput: 86.2 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 51.1%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:27:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 51.1%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:27:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 51.1%
[0;36m(APIServer pid=7765)[0;0m INFO:     127.0.0.1:37624 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:27:29 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 51.1%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:27:39 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 51.1%
[0;36m(APIServer pid=7765)[0;0m INFO 02-07 18:46:06 [launcher.py:110] Shutting down FastAPI HTTP server.
[0;36m(APIServer pid=7765)[0;0m INFO:     Shutting down
[0;36m(APIServer pid=7765)[0;0m INFO:     Waiting for application shutdown.
[0;36m(APIServer pid=7765)[0;0m INFO:     Application shutdown complete.
(APIServer pid=55063) INFO 02-07 18:47:11 [utils.py:325] 
(APIServer pid=55063) INFO 02-07 18:47:11 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
(APIServer pid=55063) INFO 02-07 18:47:11 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
(APIServer pid=55063) INFO 02-07 18:47:11 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   Qwen/Qwen2.5-3B-Instruct-AWQ
(APIServer pid=55063) INFO 02-07 18:47:11 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
(APIServer pid=55063) INFO 02-07 18:47:11 [utils.py:325] 
(APIServer pid=55063) INFO 02-07 18:47:11 [utils.py:261] non-default args: {'model_tag': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'api_server_count': 1, 'host': '0.0.0.0', 'model': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'max_model_len': 8192, 'quantization': 'awq', 'gpu_memory_utilization': 0.5}
(APIServer pid=55063) INFO 02-07 18:47:12 [model.py:541] Resolved architecture: Qwen2ForCausalLM
(APIServer pid=55063) INFO 02-07 18:47:12 [model.py:1561] Using max model len 8192
(APIServer pid=55063) INFO 02-07 18:47:12 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference
(APIServer pid=55063) INFO 02-07 18:47:12 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
(APIServer pid=55063) INFO 02-07 18:47:13 [vllm.py:624] Asynchronous scheduling is enabled.
(EngineCore_DP0 pid=56013) INFO 02-07 18:47:21 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='Qwen/Qwen2.5-3B-Instruct-AWQ', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen2.5-3B-Instruct-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
(EngineCore_DP0 pid=56013) INFO 02-07 18:47:22 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.0.76:41353 backend=nccl
(EngineCore_DP0 pid=56013) INFO 02-07 18:47:22 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
(EngineCore_DP0 pid=56013) INFO 02-07 18:47:22 [gpu_model_runner.py:4021] Starting to load model Qwen/Qwen2.5-3B-Instruct-AWQ...
(EngineCore_DP0 pid=56013) INFO 02-07 18:47:23 [cuda.py:364] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=56013) INFO 02-07 18:47:59 [weight_utils.py:527] Time spent downloading weights for Qwen/Qwen2.5-3B-Instruct-AWQ: 35.821716 seconds
(EngineCore_DP0 pid=56013) INFO 02-07 18:48:00 [weight_utils.py:567] No model.safetensors.index.json found in remote.
(EngineCore_DP0 pid=56013) Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=56013) Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.94it/s]
(EngineCore_DP0 pid=56013) Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.94it/s]
(EngineCore_DP0 pid=56013) 
(EngineCore_DP0 pid=56013) INFO 02-07 18:48:00 [default_loader.py:291] Loading weights took 0.34 seconds
(EngineCore_DP0 pid=56013) INFO 02-07 18:48:00 [gpu_model_runner.py:4118] Model loading took 1.95 GiB memory and 37.782856 seconds
(EngineCore_DP0 pid=56013) INFO 02-07 18:48:08 [backends.py:805] Using cache directory: /home/zodrak/.cache/vllm/torch_compile_cache/4d19c4f626/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=56013) INFO 02-07 18:48:08 [backends.py:865] Dynamo bytecode transform time: 7.10 s
(EngineCore_DP0 pid=56013) INFO 02-07 18:48:15 [backends.py:267] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 0.818 s
(EngineCore_DP0 pid=56013) INFO 02-07 18:48:15 [monitor.py:34] torch.compile takes 7.91 s in total
(EngineCore_DP0 pid=56013) INFO 02-07 18:48:16 [gpu_worker.py:356] Available KV cache memory: 2.43 GiB
(EngineCore_DP0 pid=56013) INFO 02-07 18:48:16 [kv_cache_utils.py:1307] GPU KV cache size: 70,784 tokens
(EngineCore_DP0 pid=56013) INFO 02-07 18:48:16 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 8.64x
(EngineCore_DP0 pid=56013) Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|â–         | 1/51 [00:00<00:12,  4.02it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 2/51 [00:00<00:12,  4.06it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 3/51 [00:00<00:10,  4.43it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 4/51 [00:00<00:10,  4.62it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|â–‰         | 5/51 [00:01<00:09,  4.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:01<00:09,  4.84it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|â–ˆâ–Ž        | 7/51 [00:01<00:08,  4.90it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 8/51 [00:01<00:08,  4.92it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 9/51 [00:01<00:08,  4.92it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–‰        | 10/51 [00:02<00:08,  4.92it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 11/51 [00:02<00:08,  4.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:02<00:07,  4.95it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:02<00:07,  5.15it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:02<00:06,  5.31it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:03<00:06,  5.42it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:03<00:06,  5.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:03<00:05,  5.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:03<00:05,  6.17it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:03<00:05,  6.39it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:03<00:04,  6.60it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:03<00:04,  6.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:04<00:04,  6.89it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:04<00:04,  6.95it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:04<00:03,  7.05it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:04<00:03,  7.26it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:04<00:03,  7.16it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:04<00:03,  7.09it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:04<00:03,  7.05it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:05<00:02,  7.33it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:05<00:02,  7.55it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:05<00:02,  7.67it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:05<00:02,  7.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:05<00:02,  8.09it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:05<00:02,  8.34it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:05<00:01,  8.47it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:05<00:01,  8.56it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [00:05<00:01,  8.51it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:06<00:01,  8.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [00:06<00:01,  9.01it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [00:06<00:01,  9.18it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [00:06<00:01,  9.40it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 43/51 [00:06<00:00,  9.78it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 45/51 [00:06<00:00,  9.95it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [00:06<00:00, 10.09it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/51 [00:07<00:00, 10.26it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:07<00:00, 10.35it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:07<00:00,  6.95it/s]
(EngineCore_DP0 pid=56013) Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   3%|â–Ž         | 1/35 [00:00<00:05,  6.23it/s]Capturing CUDA graphs (decode, FULL):   6%|â–Œ         | 2/35 [00:00<00:04,  6.65it/s]Capturing CUDA graphs (decode, FULL):   9%|â–Š         | 3/35 [00:00<00:04,  6.77it/s]Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:04,  6.87it/s]Capturing CUDA graphs (decode, FULL):  14%|â–ˆâ–        | 5/35 [00:00<00:04,  6.97it/s]Capturing CUDA graphs (decode, FULL):  17%|â–ˆâ–‹        | 6/35 [00:00<00:04,  6.98it/s]Capturing CUDA graphs (decode, FULL):  20%|â–ˆâ–ˆ        | 7/35 [00:01<00:04,  6.93it/s]Capturing CUDA graphs (decode, FULL):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:01<00:03,  7.01it/s]Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:01<00:03,  7.25it/s]Capturing CUDA graphs (decode, FULL):  29%|â–ˆâ–ˆâ–Š       | 10/35 [00:01<00:03,  7.10it/s]Capturing CUDA graphs (decode, FULL):  31%|â–ˆâ–ˆâ–ˆâ–      | 11/35 [00:01<00:03,  7.04it/s]Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:01<00:03,  6.99it/s]Capturing CUDA graphs (decode, FULL):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [00:01<00:03,  7.30it/s]Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:01<00:02,  7.53it/s]Capturing CUDA graphs (decode, FULL):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15/35 [00:02<00:02,  7.71it/s]Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:02<00:02,  7.77it/s]Capturing CUDA graphs (decode, FULL):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [00:02<00:02,  8.05it/s]Capturing CUDA graphs (decode, FULL):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:02<00:02,  8.35it/s]Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:02<00:01,  8.54it/s]Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:02<00:01,  8.70it/s]Capturing CUDA graphs (decode, FULL):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:02<00:01,  8.84it/s]Capturing CUDA graphs (decode, FULL):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 22/35 [00:02<00:01,  9.09it/s]Capturing CUDA graphs (decode, FULL):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 23/35 [00:02<00:01,  9.34it/s]Capturing CUDA graphs (decode, FULL):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25/35 [00:03<00:01,  9.80it/s]Capturing CUDA graphs (decode, FULL):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 27/35 [00:03<00:00, 10.29it/s]Capturing CUDA graphs (decode, FULL):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 29/35 [00:03<00:00, 10.45it/s]Capturing CUDA graphs (decode, FULL):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 31/35 [00:03<00:00, 10.58it/s]Capturing CUDA graphs (decode, FULL):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 33/35 [00:03<00:00, 10.72it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00, 10.84it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  8.58it/s]
(EngineCore_DP0 pid=56013) INFO 02-07 18:48:28 [gpu_model_runner.py:5051] Graph capturing finished in 12 secs, took 0.72 GiB
(EngineCore_DP0 pid=56013) INFO 02-07 18:48:28 [core.py:272] init engine (profile, create kv cache, warmup model) took 27.60 seconds
(EngineCore_DP0 pid=56013) INFO 02-07 18:49:02 [vllm.py:624] Asynchronous scheduling is enabled.
(APIServer pid=55063) INFO 02-07 18:49:02 [api_server.py:665] Supported tasks: ['generate']
(APIServer pid=55063) WARNING 02-07 18:49:03 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
(APIServer pid=55063) INFO 02-07 18:49:03 [serving.py:177] Warming up chat template processing...
(APIServer pid=55063) INFO 02-07 18:49:04 [hf.py:310] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
(APIServer pid=55063) INFO 02-07 18:49:04 [serving.py:212] Chat template warmup completed in 1566.9ms
(APIServer pid=55063) INFO 02-07 18:49:05 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:38] Available routes are:
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /docs, Methods: HEAD, GET
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /tokenize, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /detokenize, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /pause, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /resume, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /is_paused, Methods: GET
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /metrics, Methods: GET
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /health, Methods: GET
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /v1/responses, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /v1/completions, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /v1/completions/render, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /v1/messages, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /v1/models, Methods: GET
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /load, Methods: GET
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /version, Methods: GET
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /ping, Methods: GET
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /ping, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /invocations, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /classify, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /v1/embeddings, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /score, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /v1/score, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /rerank, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /v1/rerank, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /v2/rerank, Methods: POST
(APIServer pid=55063) INFO 02-07 18:49:05 [launcher.py:46] Route: /pooling, Methods: POST
(APIServer pid=55063) INFO:     Started server process [55063]
(APIServer pid=55063) INFO:     Waiting for application startup.
(APIServer pid=55063) INFO:     Application startup complete.
(APIServer pid=61787) INFO 02-07 18:49:46 [utils.py:325] 
(APIServer pid=61787) INFO 02-07 18:49:46 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
(APIServer pid=61787) INFO 02-07 18:49:46 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
(APIServer pid=61787) INFO 02-07 18:49:46 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   Qwen/Qwen2.5-3B-Instruct-AWQ
(APIServer pid=61787) INFO 02-07 18:49:46 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
(APIServer pid=61787) INFO 02-07 18:49:46 [utils.py:325] 
(APIServer pid=61787) INFO 02-07 18:49:46 [utils.py:261] non-default args: {'model_tag': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'api_server_count': 1, 'host': '0.0.0.0', 'model': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'max_model_len': 8192, 'quantization': 'awq', 'gpu_memory_utilization': 0.5}
(APIServer pid=61787) INFO 02-07 18:49:47 [model.py:541] Resolved architecture: Qwen2ForCausalLM
(APIServer pid=61787) INFO 02-07 18:49:47 [model.py:1561] Using max model len 8192
(APIServer pid=61787) INFO 02-07 18:49:48 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference
(APIServer pid=61787) INFO 02-07 18:49:48 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
(APIServer pid=61787) INFO 02-07 18:49:48 [vllm.py:624] Asynchronous scheduling is enabled.
(EngineCore_DP0 pid=62800) INFO 02-07 18:49:57 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='Qwen/Qwen2.5-3B-Instruct-AWQ', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen2.5-3B-Instruct-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
(EngineCore_DP0 pid=62800) INFO 02-07 18:49:58 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.0.76:44699 backend=nccl
(EngineCore_DP0 pid=62800) INFO 02-07 18:49:58 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
(EngineCore_DP0 pid=62800) INFO 02-07 18:49:58 [gpu_model_runner.py:4021] Starting to load model Qwen/Qwen2.5-3B-Instruct-AWQ...
(EngineCore_DP0 pid=62800) INFO 02-07 18:49:59 [cuda.py:364] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=62800) INFO 02-07 18:50:00 [weight_utils.py:567] No model.safetensors.index.json found in remote.
(EngineCore_DP0 pid=62800) Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=62800) Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.08it/s]
(EngineCore_DP0 pid=62800) Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.08it/s]
(EngineCore_DP0 pid=62800) 
(EngineCore_DP0 pid=62800) INFO 02-07 18:50:00 [default_loader.py:291] Loading weights took 0.33 seconds
(EngineCore_DP0 pid=62800) INFO 02-07 18:50:00 [gpu_model_runner.py:4118] Model loading took 1.95 GiB memory and 1.860106 seconds
(EngineCore_DP0 pid=62800) INFO 02-07 18:50:08 [backends.py:805] Using cache directory: /home/zodrak/.cache/vllm/torch_compile_cache/4d19c4f626/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=62800) INFO 02-07 18:50:08 [backends.py:865] Dynamo bytecode transform time: 7.10 s
(EngineCore_DP0 pid=62800) INFO 02-07 18:50:15 [backends.py:267] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 0.816 s
(EngineCore_DP0 pid=62800) INFO 02-07 18:50:15 [monitor.py:34] torch.compile takes 7.91 s in total
(EngineCore_DP0 pid=62800) INFO 02-07 18:50:16 [gpu_worker.py:356] Available KV cache memory: 2.43 GiB
(EngineCore_DP0 pid=62800) INFO 02-07 18:50:16 [kv_cache_utils.py:1307] GPU KV cache size: 70,784 tokens
(EngineCore_DP0 pid=62800) INFO 02-07 18:50:16 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 8.64x
(EngineCore_DP0 pid=62800) Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|â–         | 1/51 [00:00<00:12,  4.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 2/51 [00:00<00:11,  4.10it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 3/51 [00:00<00:10,  4.41it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 4/51 [00:00<00:10,  4.54it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|â–‰         | 5/51 [00:01<00:09,  4.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:01<00:09,  4.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|â–ˆâ–Ž        | 7/51 [00:01<00:09,  4.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 8/51 [00:01<00:08,  4.84it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 9/51 [00:01<00:08,  4.89it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–‰        | 10/51 [00:02<00:08,  4.91it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 11/51 [00:02<00:08,  4.94it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:02<00:07,  4.96it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:02<00:07,  5.19it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:02<00:06,  5.32it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:03<00:06,  5.44it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:03<00:06,  5.54it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:03<00:05,  5.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:03<00:05,  6.17it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:03<00:04,  6.42it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:03<00:04,  6.58it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:03<00:04,  6.69it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:04<00:04,  6.85it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:04<00:04,  6.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:04<00:03,  7.02it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:04<00:03,  7.27it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:04<00:03,  7.18it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:04<00:03,  7.10it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:04<00:03,  7.05it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:05<00:03,  7.24it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:05<00:02,  7.52it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:05<00:02,  7.67it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:05<00:02,  7.76it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:05<00:02,  8.06it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:05<00:02,  8.26it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:05<00:01,  8.44it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:05<00:01,  8.63it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [00:05<00:01,  8.46it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:06<00:01,  8.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [00:06<00:01,  9.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [00:06<00:01,  9.24it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [00:06<00:00,  9.63it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:06<00:00, 10.02it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [00:06<00:00, 10.15it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [00:07<00:00, 10.28it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [00:07<00:00, 10.42it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:07<00:00,  6.95it/s]
(EngineCore_DP0 pid=62800) Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   3%|â–Ž         | 1/35 [00:00<00:05,  6.31it/s]Capturing CUDA graphs (decode, FULL):   6%|â–Œ         | 2/35 [00:00<00:04,  6.62it/s]Capturing CUDA graphs (decode, FULL):   9%|â–Š         | 3/35 [00:00<00:04,  6.83it/s]Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:04,  6.94it/s]Capturing CUDA graphs (decode, FULL):  14%|â–ˆâ–        | 5/35 [00:00<00:04,  6.95it/s]Capturing CUDA graphs (decode, FULL):  17%|â–ˆâ–‹        | 6/35 [00:00<00:04,  7.02it/s]Capturing CUDA graphs (decode, FULL):  20%|â–ˆâ–ˆ        | 7/35 [00:01<00:03,  7.03it/s]Capturing CUDA graphs (decode, FULL):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:01<00:03,  7.10it/s]Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:01<00:03,  7.32it/s]Capturing CUDA graphs (decode, FULL):  29%|â–ˆâ–ˆâ–Š       | 10/35 [00:01<00:03,  7.21it/s]Capturing CUDA graphs (decode, FULL):  31%|â–ˆâ–ˆâ–ˆâ–      | 11/35 [00:01<00:03,  7.14it/s]Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:01<00:03,  7.05it/s]Capturing CUDA graphs (decode, FULL):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [00:01<00:02,  7.37it/s]Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:01<00:02,  7.62it/s]Capturing CUDA graphs (decode, FULL):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15/35 [00:02<00:02,  7.81it/s]Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:02<00:02,  7.83it/s]Capturing CUDA graphs (decode, FULL):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [00:02<00:02,  8.11it/s]Capturing CUDA graphs (decode, FULL):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:02<00:02,  8.38it/s]Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:02<00:01,  8.56it/s]Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:02<00:01,  8.67it/s]Capturing CUDA graphs (decode, FULL):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:02<00:01,  8.80it/s]Capturing CUDA graphs (decode, FULL):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 22/35 [00:02<00:01,  9.02it/s]Capturing CUDA graphs (decode, FULL):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 23/35 [00:02<00:01,  9.28it/s]Capturing CUDA graphs (decode, FULL):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25/35 [00:03<00:01,  9.76it/s]Capturing CUDA graphs (decode, FULL):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 27/35 [00:03<00:00, 10.28it/s]Capturing CUDA graphs (decode, FULL):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 29/35 [00:03<00:00, 10.52it/s]Capturing CUDA graphs (decode, FULL):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 31/35 [00:03<00:00, 10.68it/s]Capturing CUDA graphs (decode, FULL):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 33/35 [00:03<00:00, 10.85it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00, 11.05it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  8.65it/s]
(EngineCore_DP0 pid=62800) INFO 02-07 18:50:28 [gpu_model_runner.py:5051] Graph capturing finished in 12 secs, took 0.72 GiB
(EngineCore_DP0 pid=62800) INFO 02-07 18:50:28 [core.py:272] init engine (profile, create kv cache, warmup model) took 27.23 seconds
(EngineCore_DP0 pid=62800) INFO 02-07 18:50:29 [vllm.py:624] Asynchronous scheduling is enabled.
(APIServer pid=61787) INFO 02-07 18:50:29 [api_server.py:665] Supported tasks: ['generate']
(APIServer pid=61787) WARNING 02-07 18:50:29 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
(APIServer pid=61787) INFO 02-07 18:50:30 [serving.py:177] Warming up chat template processing...
(APIServer pid=61787) INFO 02-07 18:50:31 [hf.py:310] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
(APIServer pid=61787) INFO 02-07 18:50:31 [serving.py:212] Chat template warmup completed in 1575.7ms
(APIServer pid=61787) INFO 02-07 18:50:31 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:38] Available routes are:
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /docs, Methods: HEAD, GET
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /tokenize, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /detokenize, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /pause, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /resume, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /is_paused, Methods: GET
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /metrics, Methods: GET
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /health, Methods: GET
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /v1/responses, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /v1/completions, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /v1/completions/render, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /v1/messages, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /v1/models, Methods: GET
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /load, Methods: GET
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /version, Methods: GET
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /ping, Methods: GET
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /ping, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /invocations, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /classify, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /v1/embeddings, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /score, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /v1/score, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /rerank, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /v1/rerank, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /v2/rerank, Methods: POST
(APIServer pid=61787) INFO 02-07 18:50:31 [launcher.py:46] Route: /pooling, Methods: POST
(APIServer pid=61787) INFO:     Started server process [61787]
(APIServer pid=61787) INFO:     Waiting for application startup.
(APIServer pid=61787) INFO:     Application startup complete.
(APIServer pid=65676) INFO 02-07 18:51:58 [utils.py:325] 
(APIServer pid=65676) INFO 02-07 18:51:58 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
(APIServer pid=65676) INFO 02-07 18:51:58 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
(APIServer pid=65676) INFO 02-07 18:51:58 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   Qwen/Qwen2.5-3B-Instruct-AWQ
(APIServer pid=65676) INFO 02-07 18:51:58 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
(APIServer pid=65676) INFO 02-07 18:51:58 [utils.py:325] 
(APIServer pid=65676) INFO 02-07 18:51:58 [utils.py:261] non-default args: {'model_tag': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'api_server_count': 1, 'host': '0.0.0.0', 'model': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'max_model_len': 8192, 'quantization': 'awq', 'gpu_memory_utilization': 0.5}
(APIServer pid=65676) INFO 02-07 18:51:59 [model.py:541] Resolved architecture: Qwen2ForCausalLM
(APIServer pid=65676) INFO 02-07 18:51:59 [model.py:1561] Using max model len 8192
(APIServer pid=65676) INFO 02-07 18:51:59 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference
(APIServer pid=65676) INFO 02-07 18:51:59 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
(APIServer pid=65676) INFO 02-07 18:52:00 [vllm.py:624] Asynchronous scheduling is enabled.
(EngineCore_DP0 pid=65911) INFO 02-07 18:52:08 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='Qwen/Qwen2.5-3B-Instruct-AWQ', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen2.5-3B-Instruct-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
(EngineCore_DP0 pid=65911) INFO 02-07 18:52:08 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.0.76:54865 backend=nccl
(EngineCore_DP0 pid=65911) INFO 02-07 18:52:08 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
(EngineCore_DP0 pid=65911) INFO 02-07 18:52:09 [gpu_model_runner.py:4021] Starting to load model Qwen/Qwen2.5-3B-Instruct-AWQ...
(EngineCore_DP0 pid=65911) INFO 02-07 18:52:10 [cuda.py:364] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=65911) WARNING 02-07 18:52:20 [weight_utils.py:495] Failed to get file list for 'Qwen/Qwen2.5-3B-Instruct-AWQ'. Trying each pattern in allow_patterns individually until weights have been downloaded. Error: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 3167f425-6b50-4bce-9e99-6703ec2e0942)')
(EngineCore_DP0 pid=65911) INFO 02-07 18:52:21 [weight_utils.py:567] No model.safetensors.index.json found in remote.
(EngineCore_DP0 pid=65911) Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=65911) Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.14it/s]
(EngineCore_DP0 pid=65911) Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.14it/s]
(EngineCore_DP0 pid=65911) 
(EngineCore_DP0 pid=65911) INFO 02-07 18:52:21 [default_loader.py:291] Loading weights took 0.32 seconds
(EngineCore_DP0 pid=65911) INFO 02-07 18:52:21 [gpu_model_runner.py:4118] Model loading took 1.95 GiB memory and 11.941111 seconds
(EngineCore_DP0 pid=65911) INFO 02-07 18:52:28 [backends.py:805] Using cache directory: /home/zodrak/.cache/vllm/torch_compile_cache/4d19c4f626/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=65911) INFO 02-07 18:52:28 [backends.py:865] Dynamo bytecode transform time: 7.05 s
(EngineCore_DP0 pid=65911) INFO 02-07 18:52:36 [backends.py:267] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 0.807 s
(EngineCore_DP0 pid=65911) INFO 02-07 18:52:36 [monitor.py:34] torch.compile takes 7.86 s in total
(EngineCore_DP0 pid=65911) INFO 02-07 18:52:37 [gpu_worker.py:356] Available KV cache memory: 2.43 GiB
(EngineCore_DP0 pid=65911) INFO 02-07 18:52:37 [kv_cache_utils.py:1307] GPU KV cache size: 70,784 tokens
(EngineCore_DP0 pid=65911) INFO 02-07 18:52:37 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 8.64x
(EngineCore_DP0 pid=65911) Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|â–         | 1/51 [00:00<00:12,  4.01it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 2/51 [00:00<00:12,  4.07it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 3/51 [00:00<00:10,  4.43it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 4/51 [00:00<00:10,  4.61it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|â–‰         | 5/51 [00:01<00:09,  4.71it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:01<00:09,  4.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|â–ˆâ–Ž        | 7/51 [00:01<00:09,  4.87it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 8/51 [00:01<00:08,  4.92it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 9/51 [00:01<00:08,  4.95it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–‰        | 10/51 [00:02<00:08,  5.00it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 11/51 [00:02<00:08,  4.95it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:02<00:07,  4.97it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:02<00:07,  5.19it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:02<00:06,  5.35it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:03<00:06,  5.47it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:03<00:06,  5.55it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:03<00:05,  5.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:03<00:05,  6.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:03<00:04,  6.45it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:03<00:04,  6.65it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:03<00:04,  6.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:04<00:04,  6.81it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:04<00:04,  6.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:04<00:03,  7.01it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:04<00:03,  7.25it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:04<00:03,  7.11it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:04<00:03,  7.06it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:04<00:03,  7.02it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:05<00:03,  7.31it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:05<00:02,  7.56it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:05<00:02,  7.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:05<00:02,  7.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:05<00:02,  8.12it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:05<00:02,  8.35it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:05<00:01,  8.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:05<00:01,  8.60it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [00:05<00:01,  8.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:06<00:01,  8.73it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [00:06<00:01,  9.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [00:06<00:01,  9.25it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [00:06<00:00,  9.71it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:06<00:00, 10.12it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [00:06<00:00, 10.14it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [00:07<00:00, 10.29it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [00:07<00:00, 10.44it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:07<00:00,  6.98it/s]
(EngineCore_DP0 pid=65911) Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   3%|â–Ž         | 1/35 [00:00<00:05,  6.34it/s]Capturing CUDA graphs (decode, FULL):   6%|â–Œ         | 2/35 [00:00<00:04,  6.70it/s]Capturing CUDA graphs (decode, FULL):   9%|â–Š         | 3/35 [00:00<00:04,  6.88it/s]Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:04,  6.90it/s]Capturing CUDA graphs (decode, FULL):  14%|â–ˆâ–        | 5/35 [00:00<00:04,  6.96it/s]Capturing CUDA graphs (decode, FULL):  17%|â–ˆâ–‹        | 6/35 [00:00<00:04,  7.02it/s]Capturing CUDA graphs (decode, FULL):  20%|â–ˆâ–ˆ        | 7/35 [00:01<00:03,  7.07it/s]Capturing CUDA graphs (decode, FULL):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:01<00:03,  7.12it/s]Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:01<00:03,  7.37it/s]Capturing CUDA graphs (decode, FULL):  29%|â–ˆâ–ˆâ–Š       | 10/35 [00:01<00:03,  7.18it/s]Capturing CUDA graphs (decode, FULL):  31%|â–ˆâ–ˆâ–ˆâ–      | 11/35 [00:01<00:03,  7.10it/s]Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:01<00:03,  7.08it/s]Capturing CUDA graphs (decode, FULL):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [00:01<00:02,  7.34it/s]Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:01<00:02,  7.53it/s]Capturing CUDA graphs (decode, FULL):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15/35 [00:02<00:02,  7.63it/s]Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:02<00:02,  7.76it/s]Capturing CUDA graphs (decode, FULL):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [00:02<00:02,  8.14it/s]Capturing CUDA graphs (decode, FULL):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:02<00:02,  8.40it/s]Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:02<00:01,  8.60it/s]Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:02<00:01,  8.76it/s]Capturing CUDA graphs (decode, FULL):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:02<00:01,  8.81it/s]Capturing CUDA graphs (decode, FULL):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 22/35 [00:02<00:01,  9.09it/s]Capturing CUDA graphs (decode, FULL):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 23/35 [00:02<00:01,  9.25it/s]Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:03<00:01,  9.46it/s]Capturing CUDA graphs (decode, FULL):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26/35 [00:03<00:00, 10.04it/s]Capturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:03<00:00, 10.39it/s]Capturing CUDA graphs (decode, FULL):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 30/35 [00:03<00:00, 10.65it/s]Capturing CUDA graphs (decode, FULL):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:03<00:00, 10.79it/s]Capturing CUDA graphs (decode, FULL):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:03<00:00, 10.97it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  8.64it/s]
(EngineCore_DP0 pid=65911) INFO 02-07 18:52:49 [gpu_model_runner.py:5051] Graph capturing finished in 12 secs, took 0.72 GiB
(EngineCore_DP0 pid=65911) INFO 02-07 18:52:49 [core.py:272] init engine (profile, create kv cache, warmup model) took 27.44 seconds
(EngineCore_DP0 pid=65911) INFO 02-07 18:52:50 [vllm.py:624] Asynchronous scheduling is enabled.
(APIServer pid=65676) INFO 02-07 18:52:50 [api_server.py:665] Supported tasks: ['generate']
(APIServer pid=65676) WARNING 02-07 18:52:50 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
(APIServer pid=65676) INFO 02-07 18:52:51 [serving.py:177] Warming up chat template processing...
(APIServer pid=65676) INFO 02-07 18:52:52 [hf.py:310] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
(APIServer pid=65676) INFO 02-07 18:52:52 [serving.py:212] Chat template warmup completed in 1544.1ms
(APIServer pid=65676) INFO 02-07 18:52:52 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:38] Available routes are:
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /docs, Methods: GET, HEAD
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /tokenize, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /detokenize, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /pause, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /resume, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /is_paused, Methods: GET
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /metrics, Methods: GET
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /health, Methods: GET
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /v1/responses, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /v1/completions, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /v1/completions/render, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /v1/messages, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /v1/models, Methods: GET
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /load, Methods: GET
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /version, Methods: GET
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /ping, Methods: GET
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /ping, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /invocations, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /classify, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /v1/embeddings, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /score, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /v1/score, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /rerank, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /v1/rerank, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /v2/rerank, Methods: POST
(APIServer pid=65676) INFO 02-07 18:52:52 [launcher.py:46] Route: /pooling, Methods: POST
(APIServer pid=65676) INFO:     Started server process [65676]
(APIServer pid=65676) INFO:     Waiting for application startup.
(APIServer pid=65676) INFO:     Application startup complete.
(APIServer pid=67902) INFO 02-07 18:59:56 [utils.py:325] 
(APIServer pid=67902) INFO 02-07 18:59:56 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
(APIServer pid=67902) INFO 02-07 18:59:56 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
(APIServer pid=67902) INFO 02-07 18:59:56 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   Qwen/Qwen2.5-3B-Instruct-AWQ
(APIServer pid=67902) INFO 02-07 18:59:56 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
(APIServer pid=67902) INFO 02-07 18:59:56 [utils.py:325] 
(APIServer pid=67902) INFO 02-07 18:59:56 [utils.py:261] non-default args: {'model_tag': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'api_server_count': 1, 'host': '0.0.0.0', 'model': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'max_model_len': 8192, 'quantization': 'awq', 'gpu_memory_utilization': 0.5}
(APIServer pid=67902) INFO 02-07 19:00:02 [model.py:541] Resolved architecture: Qwen2ForCausalLM
(APIServer pid=67902) INFO 02-07 19:00:02 [model.py:1561] Using max model len 8192
(APIServer pid=67902) INFO 02-07 19:00:02 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference
(APIServer pid=67902) INFO 02-07 19:00:02 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
(APIServer pid=67902) INFO 02-07 19:00:03 [vllm.py:624] Asynchronous scheduling is enabled.
(EngineCore_DP0 pid=68307) INFO 02-07 19:00:36 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='Qwen/Qwen2.5-3B-Instruct-AWQ', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen2.5-3B-Instruct-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
(EngineCore_DP0 pid=68307) INFO 02-07 19:00:36 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.0.76:39035 backend=nccl
(EngineCore_DP0 pid=68307) INFO 02-07 19:00:36 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
(EngineCore_DP0 pid=68307) INFO 02-07 19:00:37 [gpu_model_runner.py:4021] Starting to load model Qwen/Qwen2.5-3B-Instruct-AWQ...
(EngineCore_DP0 pid=68307) INFO 02-07 19:00:37 [cuda.py:364] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=68307) INFO 02-07 19:00:38 [weight_utils.py:567] No model.safetensors.index.json found in remote.
(EngineCore_DP0 pid=68307) Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=68307) Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.18it/s]
(EngineCore_DP0 pid=68307) Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.18it/s]
(EngineCore_DP0 pid=68307) 
(EngineCore_DP0 pid=68307) INFO 02-07 19:00:39 [default_loader.py:291] Loading weights took 0.32 seconds
(EngineCore_DP0 pid=68307) INFO 02-07 19:00:39 [gpu_model_runner.py:4118] Model loading took 1.95 GiB memory and 1.847501 seconds
(EngineCore_DP0 pid=68307) INFO 02-07 19:00:47 [backends.py:805] Using cache directory: /home/zodrak/.cache/vllm/torch_compile_cache/4d19c4f626/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=68307) INFO 02-07 19:00:47 [backends.py:865] Dynamo bytecode transform time: 7.19 s
(EngineCore_DP0 pid=68307) INFO 02-07 19:00:54 [backends.py:267] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 0.823 s
(EngineCore_DP0 pid=68307) INFO 02-07 19:00:54 [monitor.py:34] torch.compile takes 8.01 s in total
(EngineCore_DP0 pid=68307) INFO 02-07 19:00:55 [gpu_worker.py:356] Available KV cache memory: 2.43 GiB
(EngineCore_DP0 pid=68307) INFO 02-07 19:00:55 [kv_cache_utils.py:1307] GPU KV cache size: 70,784 tokens
(EngineCore_DP0 pid=68307) INFO 02-07 19:00:55 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 8.64x
(EngineCore_DP0 pid=68307) Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|â–         | 1/51 [00:00<00:12,  4.02it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 2/51 [00:00<00:12,  4.05it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 3/51 [00:00<00:10,  4.38it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 4/51 [00:00<00:10,  4.56it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|â–‰         | 5/51 [00:01<00:09,  4.68it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:01<00:09,  4.76it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|â–ˆâ–Ž        | 7/51 [00:01<00:09,  4.83it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 8/51 [00:01<00:08,  4.87it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 9/51 [00:01<00:08,  4.92it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–‰        | 10/51 [00:02<00:08,  4.97it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 11/51 [00:02<00:08,  4.98it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:02<00:07,  5.01it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:02<00:07,  5.24it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:02<00:06,  5.39it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:03<00:06,  5.51it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:03<00:06,  5.57it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:03<00:05,  5.91it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:03<00:05,  6.24it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:03<00:04,  6.50it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:03<00:04,  6.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:03<00:04,  6.81it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:04<00:04,  6.90it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:04<00:04,  7.00it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:04<00:03,  7.05it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:04<00:03,  7.29it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:04<00:03,  7.16it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:04<00:03,  7.11it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:04<00:03,  7.06it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:05<00:03,  7.33it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:05<00:02,  7.60it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:05<00:02,  7.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:05<00:02,  7.82it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:05<00:02,  8.12it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:05<00:02,  8.39it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:05<00:01,  8.57it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:05<00:01,  8.69it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [00:05<00:01,  8.60it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:06<00:01,  8.87it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [00:06<00:01,  9.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [00:06<00:01,  9.26it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [00:06<00:00,  9.90it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:06<00:00, 10.23it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [00:06<00:00, 10.33it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [00:06<00:00, 10.46it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [00:07<00:00, 10.57it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:07<00:00,  7.01it/s]
(EngineCore_DP0 pid=68307) Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   3%|â–Ž         | 1/35 [00:00<00:05,  6.39it/s]Capturing CUDA graphs (decode, FULL):   6%|â–Œ         | 2/35 [00:00<00:04,  6.74it/s]Capturing CUDA graphs (decode, FULL):   9%|â–Š         | 3/35 [00:00<00:04,  6.90it/s]Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:04,  6.93it/s]Capturing CUDA graphs (decode, FULL):  14%|â–ˆâ–        | 5/35 [00:00<00:04,  6.97it/s]Capturing CUDA graphs (decode, FULL):  17%|â–ˆâ–‹        | 6/35 [00:00<00:04,  7.00it/s]Capturing CUDA graphs (decode, FULL):  20%|â–ˆâ–ˆ        | 7/35 [00:01<00:03,  7.05it/s]Capturing CUDA graphs (decode, FULL):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:01<00:03,  7.06it/s]Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:01<00:03,  7.27it/s]Capturing CUDA graphs (decode, FULL):  29%|â–ˆâ–ˆâ–Š       | 10/35 [00:01<00:03,  7.18it/s]Capturing CUDA graphs (decode, FULL):  31%|â–ˆâ–ˆâ–ˆâ–      | 11/35 [00:01<00:03,  7.12it/s]Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:01<00:03,  7.05it/s]Capturing CUDA graphs (decode, FULL):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [00:01<00:02,  7.38it/s]Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:01<00:02,  7.62it/s]Capturing CUDA graphs (decode, FULL):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15/35 [00:02<00:02,  7.76it/s]Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:02<00:02,  7.88it/s]Capturing CUDA graphs (decode, FULL):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [00:02<00:02,  8.20it/s]Capturing CUDA graphs (decode, FULL):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:02<00:02,  8.48it/s]Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:02<00:01,  8.55it/s]Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:02<00:01,  8.64it/s]Capturing CUDA graphs (decode, FULL):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:02<00:01,  8.71it/s]Capturing CUDA graphs (decode, FULL):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 22/35 [00:02<00:01,  8.93it/s]Capturing CUDA graphs (decode, FULL):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 23/35 [00:02<00:01,  9.16it/s]Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:03<00:01,  9.30it/s]Capturing CUDA graphs (decode, FULL):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26/35 [00:03<00:00,  9.91it/s]Capturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:03<00:00, 10.24it/s]Capturing CUDA graphs (decode, FULL):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 30/35 [00:03<00:00, 10.45it/s]Capturing CUDA graphs (decode, FULL):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:03<00:00, 10.59it/s]Capturing CUDA graphs (decode, FULL):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:03<00:00, 10.73it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  8.58it/s]
(EngineCore_DP0 pid=68307) INFO 02-07 19:01:07 [gpu_model_runner.py:5051] Graph capturing finished in 12 secs, took 0.72 GiB
(EngineCore_DP0 pid=68307) INFO 02-07 19:01:07 [core.py:272] init engine (profile, create kv cache, warmup model) took 27.70 seconds
(EngineCore_DP0 pid=68307) INFO 02-07 19:01:08 [vllm.py:624] Asynchronous scheduling is enabled.
(APIServer pid=67902) INFO 02-07 19:01:08 [api_server.py:665] Supported tasks: ['generate']
(APIServer pid=67902) WARNING 02-07 19:01:09 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
(APIServer pid=67902) INFO 02-07 19:01:09 [serving.py:177] Warming up chat template processing...
(APIServer pid=67902) INFO 02-07 19:01:11 [hf.py:310] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
(APIServer pid=67902) INFO 02-07 19:01:11 [serving.py:212] Chat template warmup completed in 1923.2ms
(APIServer pid=67902) INFO 02-07 19:01:11 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:38] Available routes are:
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /docs, Methods: HEAD, GET
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /tokenize, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /detokenize, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /pause, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /resume, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /is_paused, Methods: GET
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /metrics, Methods: GET
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /health, Methods: GET
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /v1/responses, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /v1/completions, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /v1/completions/render, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /v1/messages, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /v1/models, Methods: GET
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /load, Methods: GET
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /version, Methods: GET
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /ping, Methods: GET
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /ping, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /invocations, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /classify, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /v1/embeddings, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /score, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /v1/score, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /rerank, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /v1/rerank, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /v2/rerank, Methods: POST
(APIServer pid=67902) INFO 02-07 19:01:11 [launcher.py:46] Route: /pooling, Methods: POST
(APIServer pid=67902) INFO:     Started server process [67902]
(APIServer pid=67902) INFO:     Waiting for application startup.
(APIServer pid=67902) INFO:     Application startup complete.
(APIServer pid=68975) INFO 02-07 19:02:05 [utils.py:325] 
(APIServer pid=68975) INFO 02-07 19:02:05 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
(APIServer pid=68975) INFO 02-07 19:02:05 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
(APIServer pid=68975) INFO 02-07 19:02:05 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   Qwen/Qwen2.5-3B-Instruct-AWQ
(APIServer pid=68975) INFO 02-07 19:02:05 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
(APIServer pid=68975) INFO 02-07 19:02:05 [utils.py:325] 
(APIServer pid=68975) INFO 02-07 19:02:05 [utils.py:261] non-default args: {'model_tag': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'api_server_count': 1, 'host': '0.0.0.0', 'model': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'max_model_len': 8192, 'quantization': 'awq', 'gpu_memory_utilization': 0.5}
(APIServer pid=68975) INFO 02-07 19:02:06 [model.py:541] Resolved architecture: Qwen2ForCausalLM
(APIServer pid=68975) INFO 02-07 19:02:06 [model.py:1561] Using max model len 8192
(APIServer pid=68975) INFO 02-07 19:02:06 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference
(APIServer pid=68975) INFO 02-07 19:02:06 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
(APIServer pid=68975) INFO 02-07 19:02:07 [vllm.py:624] Asynchronous scheduling is enabled.
(EngineCore_DP0 pid=69371) INFO 02-07 19:02:46 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='Qwen/Qwen2.5-3B-Instruct-AWQ', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen2.5-3B-Instruct-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
(EngineCore_DP0 pid=69371) INFO 02-07 19:02:46 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.0.76:56905 backend=nccl
(EngineCore_DP0 pid=69371) INFO 02-07 19:02:46 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
(EngineCore_DP0 pid=69371) INFO 02-07 19:02:46 [gpu_model_runner.py:4021] Starting to load model Qwen/Qwen2.5-3B-Instruct-AWQ...
(EngineCore_DP0 pid=69371) INFO 02-07 19:02:47 [cuda.py:364] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=69371) INFO 02-07 19:03:18 [weight_utils.py:527] Time spent downloading weights for Qwen/Qwen2.5-3B-Instruct-AWQ: 30.164101 seconds
(EngineCore_DP0 pid=69371) INFO 02-07 19:03:18 [weight_utils.py:567] No model.safetensors.index.json found in remote.
(EngineCore_DP0 pid=69371) Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=69371) Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.06it/s]
(EngineCore_DP0 pid=69371) Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.06it/s]
(EngineCore_DP0 pid=69371) 
(EngineCore_DP0 pid=69371) INFO 02-07 19:03:18 [default_loader.py:291] Loading weights took 0.33 seconds
(EngineCore_DP0 pid=69371) INFO 02-07 19:03:19 [gpu_model_runner.py:4118] Model loading took 1.95 GiB memory and 31.904724 seconds
(EngineCore_DP0 pid=69371) INFO 02-07 19:03:26 [backends.py:805] Using cache directory: /home/zodrak/.cache/vllm/torch_compile_cache/4d19c4f626/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=69371) INFO 02-07 19:03:26 [backends.py:865] Dynamo bytecode transform time: 7.04 s
(EngineCore_DP0 pid=69371) INFO 02-07 19:03:34 [backends.py:267] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 0.807 s
(EngineCore_DP0 pid=69371) INFO 02-07 19:03:34 [monitor.py:34] torch.compile takes 7.84 s in total
(EngineCore_DP0 pid=69371) INFO 02-07 19:03:35 [gpu_worker.py:356] Available KV cache memory: 2.43 GiB
(EngineCore_DP0 pid=69371) INFO 02-07 19:03:35 [kv_cache_utils.py:1307] GPU KV cache size: 70,784 tokens
(EngineCore_DP0 pid=69371) INFO 02-07 19:03:35 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 8.64x
(EngineCore_DP0 pid=69371) Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|â–         | 1/51 [00:00<00:12,  4.07it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 2/51 [00:00<00:11,  4.12it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 3/51 [00:00<00:10,  4.43it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 4/51 [00:00<00:10,  4.61it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|â–‰         | 5/51 [00:01<00:09,  4.73it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:01<00:09,  4.82it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|â–ˆâ–Ž        | 7/51 [00:01<00:09,  4.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 8/51 [00:01<00:08,  4.92it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 9/51 [00:01<00:08,  4.96it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–‰        | 10/51 [00:02<00:08,  4.98it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 11/51 [00:02<00:08,  5.00it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:02<00:07,  5.02it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:02<00:07,  5.22it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:02<00:06,  5.38it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:03<00:06,  5.46it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:03<00:06,  5.56it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:03<00:05,  5.92it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:03<00:05,  6.20it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:03<00:04,  6.46it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:03<00:04,  6.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:03<00:04,  6.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:04<00:04,  6.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:04<00:04,  6.95it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:04<00:03,  7.06it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:04<00:03,  7.31it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:04<00:03,  7.20it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:04<00:03,  7.12it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:04<00:03,  7.09it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:04<00:02,  7.39it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:05<00:02,  7.63it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:05<00:02,  7.71it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:05<00:02,  7.78it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:05<00:02,  8.14it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:05<00:02,  8.33it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:05<00:01,  8.50it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:05<00:01,  8.67it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [00:05<00:01,  8.50it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:06<00:01,  8.78it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [00:06<00:01,  9.08it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [00:06<00:01,  9.29it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [00:06<00:00,  9.73it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:06<00:00, 10.10it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [00:06<00:00, 10.28it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [00:06<00:00, 10.39it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [00:07<00:00, 10.55it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:07<00:00,  7.02it/s]
(EngineCore_DP0 pid=69371) Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   3%|â–Ž         | 1/35 [00:00<00:05,  6.30it/s]Capturing CUDA graphs (decode, FULL):   6%|â–Œ         | 2/35 [00:00<00:04,  6.72it/s]Capturing CUDA graphs (decode, FULL):   9%|â–Š         | 3/35 [00:00<00:04,  6.87it/s]Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:04,  6.90it/s]Capturing CUDA graphs (decode, FULL):  14%|â–ˆâ–        | 5/35 [00:00<00:04,  6.97it/s]Capturing CUDA graphs (decode, FULL):  17%|â–ˆâ–‹        | 6/35 [00:00<00:04,  7.00it/s]Capturing CUDA graphs (decode, FULL):  20%|â–ˆâ–ˆ        | 7/35 [00:01<00:03,  7.06it/s]Capturing CUDA graphs (decode, FULL):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:01<00:03,  7.11it/s]Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:01<00:03,  7.36it/s]Capturing CUDA graphs (decode, FULL):  29%|â–ˆâ–ˆâ–Š       | 10/35 [00:01<00:03,  7.21it/s]Capturing CUDA graphs (decode, FULL):  31%|â–ˆâ–ˆâ–ˆâ–      | 11/35 [00:01<00:03,  7.13it/s]Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:01<00:03,  7.09it/s]Capturing CUDA graphs (decode, FULL):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [00:01<00:03,  7.33it/s]Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:01<00:02,  7.58it/s]Capturing CUDA graphs (decode, FULL):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15/35 [00:02<00:02,  7.75it/s]Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:02<00:02,  7.84it/s]Capturing CUDA graphs (decode, FULL):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [00:02<00:02,  8.17it/s]Capturing CUDA graphs (decode, FULL):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:02<00:02,  8.46it/s]Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:02<00:01,  8.64it/s]Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:02<00:01,  8.74it/s]Capturing CUDA graphs (decode, FULL):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:02<00:01,  8.85it/s]Capturing CUDA graphs (decode, FULL):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 22/35 [00:02<00:01,  9.13it/s]Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:03<00:01,  9.55it/s]Capturing CUDA graphs (decode, FULL):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26/35 [00:03<00:00, 10.07it/s]Capturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:03<00:00, 10.38it/s]Capturing CUDA graphs (decode, FULL):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 30/35 [00:03<00:00, 10.63it/s]Capturing CUDA graphs (decode, FULL):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:03<00:00, 10.75it/s]Capturing CUDA graphs (decode, FULL):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:03<00:00, 10.94it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  8.68it/s]
(EngineCore_DP0 pid=69371) INFO 02-07 19:03:47 [gpu_model_runner.py:5051] Graph capturing finished in 12 secs, took 0.72 GiB
(EngineCore_DP0 pid=69371) INFO 02-07 19:03:47 [core.py:272] init engine (profile, create kv cache, warmup model) took 27.75 seconds
(EngineCore_DP0 pid=69371) INFO 02-07 19:03:48 [vllm.py:624] Asynchronous scheduling is enabled.
(APIServer pid=68975) INFO 02-07 19:03:48 [api_server.py:665] Supported tasks: ['generate']
(APIServer pid=68975) WARNING 02-07 19:03:48 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
(APIServer pid=68975) INFO 02-07 19:03:49 [serving.py:177] Warming up chat template processing...
(APIServer pid=68975) INFO 02-07 19:03:50 [hf.py:310] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
(APIServer pid=68975) INFO 02-07 19:03:50 [serving.py:212] Chat template warmup completed in 1559.4ms
(APIServer pid=68975) INFO 02-07 19:03:50 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:38] Available routes are:
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /docs, Methods: GET, HEAD
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /tokenize, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /detokenize, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /pause, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /resume, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /is_paused, Methods: GET
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /metrics, Methods: GET
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /health, Methods: GET
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /v1/responses, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /v1/completions, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /v1/completions/render, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /v1/messages, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /v1/models, Methods: GET
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /load, Methods: GET
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /version, Methods: GET
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /ping, Methods: GET
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /ping, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /invocations, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /classify, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /v1/embeddings, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /score, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /v1/score, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /rerank, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /v1/rerank, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /v2/rerank, Methods: POST
(APIServer pid=68975) INFO 02-07 19:03:50 [launcher.py:46] Route: /pooling, Methods: POST
(APIServer pid=68975) INFO:     Started server process [68975]
(APIServer pid=68975) INFO:     Waiting for application startup.
(APIServer pid=68975) INFO:     Application startup complete.
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:29:49 [utils.py:325] 
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:29:49 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:29:49 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:29:49 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   Qwen/Qwen2.5-3B-Instruct-AWQ
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:29:49 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:29:49 [utils.py:325] 
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:29:49 [utils.py:261] non-default args: {'model_tag': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'api_server_count': 1, 'host': '0.0.0.0', 'model': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'max_model_len': 8192, 'quantization': 'awq', 'gpu_memory_utilization': 0.5}
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:29:50 [model.py:541] Resolved architecture: Qwen2ForCausalLM
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:29:50 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:29:50 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:29:50 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:29:51 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=74723)[0;0m INFO 02-07 19:30:00 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='Qwen/Qwen2.5-3B-Instruct-AWQ', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen2.5-3B-Instruct-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=74723)[0;0m INFO 02-07 19:30:00 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.0.76:40699 backend=nccl
[0;36m(EngineCore_DP0 pid=74723)[0;0m INFO 02-07 19:30:00 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=74723)[0;0m INFO 02-07 19:30:01 [gpu_model_runner.py:4021] Starting to load model Qwen/Qwen2.5-3B-Instruct-AWQ...
[0;36m(EngineCore_DP0 pid=74723)[0;0m INFO 02-07 19:30:01 [cuda.py:364] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=74723)[0;0m INFO 02-07 19:30:03 [weight_utils.py:567] No model.safetensors.index.json found in remote.
[0;36m(EngineCore_DP0 pid=74723)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=74723)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.30it/s]
[0;36m(EngineCore_DP0 pid=74723)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.30it/s]
[0;36m(EngineCore_DP0 pid=74723)[0;0m 
[0;36m(EngineCore_DP0 pid=74723)[0;0m INFO 02-07 19:30:03 [default_loader.py:291] Loading weights took 0.31 seconds
[0;36m(EngineCore_DP0 pid=74723)[0;0m INFO 02-07 19:30:03 [gpu_model_runner.py:4118] Model loading took 1.95 GiB memory and 2.170547 seconds
[0;36m(EngineCore_DP0 pid=74723)[0;0m INFO 02-07 19:30:11 [backends.py:805] Using cache directory: /home/zodrak/.cache/vllm/torch_compile_cache/4d19c4f626/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=74723)[0;0m INFO 02-07 19:30:11 [backends.py:865] Dynamo bytecode transform time: 7.10 s
[0;36m(EngineCore_DP0 pid=74723)[0;0m INFO 02-07 19:30:19 [backends.py:267] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 0.780 s
[0;36m(EngineCore_DP0 pid=74723)[0;0m INFO 02-07 19:30:19 [monitor.py:34] torch.compile takes 7.88 s in total
[0;36m(EngineCore_DP0 pid=74723)[0;0m INFO 02-07 19:30:19 [gpu_worker.py:356] Available KV cache memory: 2.39 GiB
[0;36m(EngineCore_DP0 pid=74723)[0;0m INFO 02-07 19:30:19 [kv_cache_utils.py:1307] GPU KV cache size: 69,696 tokens
[0;36m(EngineCore_DP0 pid=74723)[0;0m INFO 02-07 19:30:19 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 8.51x
[0;36m(EngineCore_DP0 pid=74723)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|â–         | 1/51 [00:00<00:12,  4.13it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 2/51 [00:00<00:11,  4.17it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 3/51 [00:00<00:10,  4.54it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 4/51 [00:00<00:09,  4.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|â–‰         | 5/51 [00:01<00:09,  4.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:01<00:08,  5.04it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|â–ˆâ–Ž        | 7/51 [00:01<00:08,  4.97it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 8/51 [00:01<00:08,  5.06it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 9/51 [00:01<00:08,  5.10it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–‰        | 10/51 [00:02<00:08,  5.12it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 11/51 [00:02<00:07,  5.09it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:02<00:07,  5.09it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:02<00:07,  5.30it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:02<00:06,  5.44it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:02<00:06,  5.57it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:03<00:06,  5.68it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:03<00:05,  6.05it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:03<00:05,  6.39it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:03<00:04,  6.65it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:03<00:04,  6.86it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:03<00:04,  7.00it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:03<00:04,  7.07it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:04<00:03,  7.11it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:04<00:03,  7.15it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:04<00:03,  7.38it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:04<00:03,  7.27it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:04<00:03,  7.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:04<00:03,  7.17it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:04<00:02,  7.47it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:05<00:02,  7.69it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:05<00:02,  7.83it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:05<00:02,  7.90it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:05<00:02,  8.22it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:05<00:02,  8.47it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:05<00:01,  8.67it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:05<00:01,  8.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [00:05<00:01,  8.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:05<00:01,  9.10it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [00:06<00:01,  9.33it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [00:06<00:01,  9.45it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [00:06<00:00,  9.95it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:06<00:00, 10.22it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [00:06<00:00, 10.37it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [00:06<00:00, 10.56it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [00:07<00:00, 10.86it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:07<00:00,  7.17it/s]
[0;36m(EngineCore_DP0 pid=74723)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   3%|â–Ž         | 1/35 [00:00<00:05,  6.40it/s]Capturing CUDA graphs (decode, FULL):   6%|â–Œ         | 2/35 [00:00<00:04,  6.84it/s]Capturing CUDA graphs (decode, FULL):   9%|â–Š         | 3/35 [00:00<00:04,  6.97it/s]Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:04,  7.00it/s]Capturing CUDA graphs (decode, FULL):  14%|â–ˆâ–        | 5/35 [00:00<00:04,  7.02it/s]Capturing CUDA graphs (decode, FULL):  17%|â–ˆâ–‹        | 6/35 [00:00<00:04,  7.10it/s]Capturing CUDA graphs (decode, FULL):  20%|â–ˆâ–ˆ        | 7/35 [00:00<00:03,  7.12it/s]Capturing CUDA graphs (decode, FULL):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:01<00:03,  7.25it/s]Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:01<00:03,  7.53it/s]Capturing CUDA graphs (decode, FULL):  29%|â–ˆâ–ˆâ–Š       | 10/35 [00:01<00:03,  7.34it/s]Capturing CUDA graphs (decode, FULL):  31%|â–ˆâ–ˆâ–ˆâ–      | 11/35 [00:01<00:03,  7.25it/s]Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:01<00:03,  7.15it/s]Capturing CUDA graphs (decode, FULL):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [00:01<00:02,  7.38it/s]Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:01<00:02,  7.57it/s]Capturing CUDA graphs (decode, FULL):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15/35 [00:02<00:02,  7.69it/s]Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:02<00:02,  7.78it/s]Capturing CUDA graphs (decode, FULL):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [00:02<00:02,  8.11it/s]Capturing CUDA graphs (decode, FULL):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:02<00:02,  8.36it/s]Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:02<00:01,  8.50it/s]Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:02<00:01,  8.64it/s]Capturing CUDA graphs (decode, FULL):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:02<00:01,  8.72it/s]Capturing CUDA graphs (decode, FULL):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 22/35 [00:02<00:01,  8.97it/s]Capturing CUDA graphs (decode, FULL):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 23/35 [00:02<00:01,  9.21it/s]Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:03<00:01,  9.40it/s]Capturing CUDA graphs (decode, FULL):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26/35 [00:03<00:00, 10.14it/s]Capturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:03<00:00, 10.56it/s]Capturing CUDA graphs (decode, FULL):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 30/35 [00:03<00:00, 10.85it/s]Capturing CUDA graphs (decode, FULL):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:03<00:00, 10.91it/s]Capturing CUDA graphs (decode, FULL):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:03<00:00, 10.94it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  8.69it/s]
[0;36m(EngineCore_DP0 pid=74723)[0;0m INFO 02-07 19:30:31 [gpu_model_runner.py:5051] Graph capturing finished in 12 secs, took 0.76 GiB
[0;36m(EngineCore_DP0 pid=74723)[0;0m INFO 02-07 19:30:31 [core.py:272] init engine (profile, create kv cache, warmup model) took 27.87 seconds
[0;36m(EngineCore_DP0 pid=74723)[0;0m INFO 02-07 19:30:33 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:33 [api_server.py:665] Supported tasks: ['generate']
[0;36m(APIServer pid=74486)[0;0m WARNING 02-07 19:30:33 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:33 [serving.py:177] Warming up chat template processing...
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [hf.py:310] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [serving.py:212] Chat template warmup completed in 1640.9ms
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:30:35 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=74486)[0;0m INFO:     Started server process [74486]
[0;36m(APIServer pid=74486)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=74486)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:31:33 [serving.py:323] Error in preprocessing prompt inputs
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:31:33 [serving.py:323] Traceback (most recent call last):
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:31:33 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:31:33 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:31:33 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:31:33 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:31:33 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:31:33 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:31:33 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:31:33 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:31:33 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:31:33 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:31:33 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:31:33 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:31:33 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:31:33 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:31:33 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:31:33 [serving.py:323]     raise VLLMValidationError(
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:31:33 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 2048. This model's maximum context length is 8192 tokens and your request has 6906 input tokens (2048 > 8192 - 6906). (parameter=max_tokens, value=2048)
[0;36m(APIServer pid=74486)[0;0m INFO:     127.0.0.1:36998 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:31:45 [loggers.py:257] Engine 000: Avg prompt throughput: 690.6 tokens/s, Avg generation throughput: 13.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=74486)[0;0m INFO:     127.0.0.1:37000 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:31:55 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=74486)[0;0m INFO:     127.0.0.1:33960 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:32:05 [loggers.py:257] Engine 000: Avg prompt throughput: 23.2 tokens/s, Avg generation throughput: 11.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.2%
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:32:15 [loggers.py:257] Engine 000: Avg prompt throughput: 62.9 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=74486)[0;0m INFO:     127.0.0.1:52562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:32:25 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:32:35 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:37:09 [serving.py:323] Error in preprocessing prompt inputs
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:37:09 [serving.py:323] Traceback (most recent call last):
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:37:09 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:37:09 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:37:09 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:37:09 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:37:09 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:37:09 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:37:09 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:37:09 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:37:09 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:37:09 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:37:09 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:37:09 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:37:09 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:37:09 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:37:09 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:37:09 [serving.py:323]     raise VLLMValidationError(
[0;36m(APIServer pid=74486)[0;0m ERROR 02-07 19:37:09 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 2048. This model's maximum context length is 8192 tokens and your request has 6909 input tokens (2048 > 8192 - 6909). (parameter=max_tokens, value=2048)
[0;36m(APIServer pid=74486)[0;0m INFO:     127.0.0.1:54686 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:37:15 [loggers.py:257] Engine 000: Avg prompt throughput: 690.9 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.0%, Prefix cache hit rate: 23.5%
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:37:25 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.2%, Prefix cache hit rate: 23.5%
[0;36m(APIServer pid=74486)[0;0m INFO:     127.0.0.1:54694 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:37:35 [loggers.py:257] Engine 000: Avg prompt throughput: 23.2 tokens/s, Avg generation throughput: 11.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 24.7%
[0;36m(APIServer pid=74486)[0;0m INFO:     127.0.0.1:36290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:37:45 [loggers.py:257] Engine 000: Avg prompt throughput: 62.9 tokens/s, Avg generation throughput: 14.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 27.7%
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:37:55 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 27.7%
[0;36m(APIServer pid=74486)[0;0m INFO:     127.0.0.1:55846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:38:05 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 27.7%
[0;36m(APIServer pid=74486)[0;0m INFO 02-07 19:38:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 27.7%
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:02 [utils.py:325] 
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:02 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:02 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:02 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   Qwen/Qwen2.5-3B-Instruct-AWQ
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:02 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:02 [utils.py:325] 
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:02 [utils.py:261] non-default args: {'model_tag': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'api_server_count': 1, 'host': '0.0.0.0', 'model': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'max_model_len': 8192, 'quantization': 'awq', 'gpu_memory_utilization': 0.5}
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:03 [model.py:541] Resolved architecture: Qwen2ForCausalLM
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:03 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:03 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:04 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:04 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=8261)[0;0m INFO 02-08 09:43:13 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='Qwen/Qwen2.5-3B-Instruct-AWQ', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen2.5-3B-Instruct-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=8261)[0;0m INFO 02-08 09:43:13 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.0.76:37347 backend=nccl
[0;36m(EngineCore_DP0 pid=8261)[0;0m INFO 02-08 09:43:13 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=8261)[0;0m INFO 02-08 09:43:14 [gpu_model_runner.py:4021] Starting to load model Qwen/Qwen2.5-3B-Instruct-AWQ...
[0;36m(EngineCore_DP0 pid=8261)[0;0m INFO 02-08 09:43:15 [cuda.py:364] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=8261)[0;0m INFO 02-08 09:43:16 [weight_utils.py:567] No model.safetensors.index.json found in remote.
[0;36m(EngineCore_DP0 pid=8261)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=8261)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.28s/it]
[0;36m(EngineCore_DP0 pid=8261)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.28s/it]
[0;36m(EngineCore_DP0 pid=8261)[0;0m 
[0;36m(EngineCore_DP0 pid=8261)[0;0m INFO 02-08 09:43:17 [default_loader.py:291] Loading weights took 1.29 seconds
[0;36m(EngineCore_DP0 pid=8261)[0;0m INFO 02-08 09:43:17 [gpu_model_runner.py:4118] Model loading took 1.95 GiB memory and 3.135448 seconds
[0;36m(EngineCore_DP0 pid=8261)[0;0m INFO 02-08 09:43:25 [backends.py:805] Using cache directory: /home/zodrak/.cache/vllm/torch_compile_cache/4d19c4f626/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=8261)[0;0m INFO 02-08 09:43:25 [backends.py:865] Dynamo bytecode transform time: 7.90 s
[0;36m(EngineCore_DP0 pid=8261)[0;0m INFO 02-08 09:43:34 [backends.py:267] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 0.840 s
[0;36m(EngineCore_DP0 pid=8261)[0;0m INFO 02-08 09:43:34 [monitor.py:34] torch.compile takes 8.74 s in total
[0;36m(EngineCore_DP0 pid=8261)[0;0m INFO 02-08 09:43:34 [gpu_worker.py:356] Available KV cache memory: 2.4 GiB
[0;36m(EngineCore_DP0 pid=8261)[0;0m INFO 02-08 09:43:34 [kv_cache_utils.py:1307] GPU KV cache size: 70,016 tokens
[0;36m(EngineCore_DP0 pid=8261)[0;0m INFO 02-08 09:43:34 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 8.55x
[0;36m(EngineCore_DP0 pid=8261)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|â–         | 1/51 [00:00<00:12,  4.06it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 2/51 [00:00<00:11,  4.18it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 3/51 [00:00<00:10,  4.54it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 4/51 [00:00<00:09,  4.73it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|â–‰         | 5/51 [00:01<00:09,  4.87it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:01<00:09,  4.97it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|â–ˆâ–Ž        | 7/51 [00:01<00:08,  5.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 8/51 [00:01<00:08,  5.09it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 9/51 [00:01<00:08,  5.14it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–‰        | 10/51 [00:02<00:07,  5.17it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 11/51 [00:02<00:07,  5.19it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:02<00:07,  5.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:02<00:06,  5.43it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:02<00:06,  5.59it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:02<00:06,  5.71it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:03<00:06,  5.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:03<00:05,  6.15it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:03<00:05,  6.47it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:03<00:04,  6.71it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:03<00:04,  6.91it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:03<00:04,  7.04it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:03<00:04,  7.19it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:04<00:03,  7.28it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:04<00:03,  7.35it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:04<00:03,  7.59it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:04<00:03,  7.45it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:04<00:03,  7.39it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:04<00:03,  7.32it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:04<00:02,  7.63it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:04<00:02,  7.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:05<00:02,  8.04it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:05<00:02,  8.07it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:05<00:02,  8.28it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:05<00:01,  8.52it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:05<00:01,  8.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:05<00:01,  8.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [00:05<00:01,  8.71it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:05<00:01,  9.04it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [00:05<00:01,  9.26it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [00:06<00:01,  9.73it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 43/51 [00:06<00:00, 10.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 45/51 [00:06<00:00, 10.46it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [00:06<00:00, 10.63it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/51 [00:06<00:00, 10.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:07<00:00, 10.91it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:07<00:00,  7.25it/s]
[0;36m(EngineCore_DP0 pid=8261)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   3%|â–Ž         | 1/35 [00:00<00:05,  6.37it/s]Capturing CUDA graphs (decode, FULL):   6%|â–Œ         | 2/35 [00:00<00:04,  6.77it/s]Capturing CUDA graphs (decode, FULL):   9%|â–Š         | 3/35 [00:00<00:04,  7.01it/s]Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:04,  7.14it/s]Capturing CUDA graphs (decode, FULL):  14%|â–ˆâ–        | 5/35 [00:00<00:04,  7.22it/s]Capturing CUDA graphs (decode, FULL):  17%|â–ˆâ–‹        | 6/35 [00:00<00:03,  7.26it/s]Capturing CUDA graphs (decode, FULL):  20%|â–ˆâ–ˆ        | 7/35 [00:00<00:03,  7.30it/s]Capturing CUDA graphs (decode, FULL):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:01<00:03,  7.36it/s]Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:01<00:03,  7.55it/s]Capturing CUDA graphs (decode, FULL):  29%|â–ˆâ–ˆâ–Š       | 10/35 [00:01<00:03,  7.34it/s]Capturing CUDA graphs (decode, FULL):  31%|â–ˆâ–ˆâ–ˆâ–      | 11/35 [00:01<00:03,  7.20it/s]Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:01<00:03,  7.16it/s]Capturing CUDA graphs (decode, FULL):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [00:01<00:02,  7.48it/s]Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:01<00:02,  7.74it/s]Capturing CUDA graphs (decode, FULL):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15/35 [00:02<00:02,  7.94it/s]Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:02<00:02,  8.01it/s]Capturing CUDA graphs (decode, FULL):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [00:02<00:02,  8.35it/s]Capturing CUDA graphs (decode, FULL):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:02<00:01,  8.61it/s]Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:02<00:01,  8.73it/s]Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:02<00:01,  8.91it/s]Capturing CUDA graphs (decode, FULL):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:02<00:01,  8.99it/s]Capturing CUDA graphs (decode, FULL):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 22/35 [00:02<00:01,  9.24it/s]Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:02<00:01,  9.58it/s]Capturing CUDA graphs (decode, FULL):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26/35 [00:03<00:00, 10.05it/s]Capturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:03<00:00, 10.43it/s]Capturing CUDA graphs (decode, FULL):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 30/35 [00:03<00:00, 10.74it/s]Capturing CUDA graphs (decode, FULL):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:03<00:00, 10.89it/s]Capturing CUDA graphs (decode, FULL):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:03<00:00, 11.13it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:03<00:00,  8.83it/s]
[0;36m(EngineCore_DP0 pid=8261)[0;0m INFO 02-08 09:43:46 [gpu_model_runner.py:5051] Graph capturing finished in 11 secs, took 0.74 GiB
[0;36m(EngineCore_DP0 pid=8261)[0;0m INFO 02-08 09:43:46 [core.py:272] init engine (profile, create kv cache, warmup model) took 28.55 seconds
[0;36m(EngineCore_DP0 pid=8261)[0;0m INFO 02-08 09:43:47 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:47 [api_server.py:665] Supported tasks: ['generate']
[0;36m(APIServer pid=7682)[0;0m WARNING 02-08 09:43:48 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:48 [serving.py:177] Warming up chat template processing...
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:49 [hf.py:310] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:49 [serving.py:212] Chat template warmup completed in 1545.5ms
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:43:50 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=7682)[0;0m INFO:     Started server process [7682]
[0;36m(APIServer pid=7682)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=7682)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:44:00 [loggers.py:257] Engine 000: Avg prompt throughput: 13.4 tokens/s, Avg generation throughput: 1.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=7682)[0;0m INFO:     127.0.0.1:48268 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:44:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:44:20 [loggers.py:257] Engine 000: Avg prompt throughput: 62.6 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 2.0%
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:44:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 2.0%
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:44:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 2.0%
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:44:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 2.0%
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:45:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 2.0%
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:45:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 2.0%
[0;36m(APIServer pid=7682)[0;0m INFO:     127.0.0.1:42818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:45:20 [loggers.py:257] Engine 000: Avg prompt throughput: 56.6 tokens/s, Avg generation throughput: 15.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 2.4%
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:45:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 2.4%
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:45:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 2.4%
[0;36m(APIServer pid=7682)[0;0m INFO:     127.0.0.1:35430 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:45:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 4.1%
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:46:00 [loggers.py:257] Engine 000: Avg prompt throughput: 101.0 tokens/s, Avg generation throughput: 14.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 4.1%
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:46:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 4.1%
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:46:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 4.1%
[0;36m(APIServer pid=7682)[0;0m INFO:     127.0.0.1:35592 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:46:30 [loggers.py:257] Engine 000: Avg prompt throughput: 337.3 tokens/s, Avg generation throughput: 13.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.9%, Prefix cache hit rate: 2.0%
[0;36m(APIServer pid=7682)[0;0m INFO:     127.0.0.1:41440 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:46:40 [loggers.py:257] Engine 000: Avg prompt throughput: 366.8 tokens/s, Avg generation throughput: 13.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.3%, Prefix cache hit rate: 1.4%
[0;36m(APIServer pid=7682)[0;0m INFO:     127.0.0.1:38854 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:46:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.4%
[0;36m(APIServer pid=7682)[0;0m INFO:     127.0.0.1:56474 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:47:00 [loggers.py:257] Engine 000: Avg prompt throughput: 677.5 tokens/s, Avg generation throughput: 11.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.9%, Prefix cache hit rate: 1.0%
[0;36m(APIServer pid=7682)[0;0m INFO:     127.0.0.1:57262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:47:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.0%
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:47:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.0%
[0;36m(APIServer pid=7682)[0;0m ERROR 02-08 09:47:58 [serving.py:323] Error in preprocessing prompt inputs
[0;36m(APIServer pid=7682)[0;0m ERROR 02-08 09:47:58 [serving.py:323] Traceback (most recent call last):
[0;36m(APIServer pid=7682)[0;0m ERROR 02-08 09:47:58 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
[0;36m(APIServer pid=7682)[0;0m ERROR 02-08 09:47:58 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
[0;36m(APIServer pid=7682)[0;0m ERROR 02-08 09:47:58 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7682)[0;0m ERROR 02-08 09:47:58 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
[0;36m(APIServer pid=7682)[0;0m ERROR 02-08 09:47:58 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
[0;36m(APIServer pid=7682)[0;0m ERROR 02-08 09:47:58 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7682)[0;0m ERROR 02-08 09:47:58 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
[0;36m(APIServer pid=7682)[0;0m ERROR 02-08 09:47:58 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
[0;36m(APIServer pid=7682)[0;0m ERROR 02-08 09:47:58 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
[0;36m(APIServer pid=7682)[0;0m ERROR 02-08 09:47:58 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
[0;36m(APIServer pid=7682)[0;0m ERROR 02-08 09:47:58 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7682)[0;0m ERROR 02-08 09:47:58 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
[0;36m(APIServer pid=7682)[0;0m ERROR 02-08 09:47:58 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
[0;36m(APIServer pid=7682)[0;0m ERROR 02-08 09:47:58 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=7682)[0;0m ERROR 02-08 09:47:58 [serving.py:323]   File "/home/zodrak/.pyenv/versions/3.12.8/envs/py312/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
[0;36m(APIServer pid=7682)[0;0m ERROR 02-08 09:47:58 [serving.py:323]     raise VLLMValidationError(
[0;36m(APIServer pid=7682)[0;0m ERROR 02-08 09:47:58 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 2048. This model's maximum context length is 8192 tokens and your request has 6893 input tokens (2048 > 8192 - 6893). (parameter=max_tokens, value=2048)
[0;36m(APIServer pid=7682)[0;0m INFO:     127.0.0.1:46824 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:48:10 [loggers.py:257] Engine 000: Avg prompt throughput: 689.3 tokens/s, Avg generation throughput: 12.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.0%, Prefix cache hit rate: 0.8%
[0;36m(APIServer pid=7682)[0;0m INFO:     127.0.0.1:46834 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:48:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.8%
[0;36m(APIServer pid=7682)[0;0m INFO:     127.0.0.1:57668 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:48:30 [loggers.py:257] Engine 000: Avg prompt throughput: 84.7 tokens/s, Avg generation throughput: 13.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.9%
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:48:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.9%
[0;36m(APIServer pid=7682)[0;0m INFO:     127.0.0.1:57272 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:48:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.9%
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:49:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.9%
[0;36m(APIServer pid=7682)[0;0m INFO 02-08 09:58:45 [launcher.py:110] Shutting down FastAPI HTTP server.
[0;36m(APIServer pid=7682)[0;0m INFO:     Shutting down
[0;36m(APIServer pid=7682)[0;0m INFO:     Waiting for application shutdown.
[0;36m(APIServer pid=7682)[0;0m INFO:     Application shutdown complete.
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 09:59:36 [utils.py:325] 
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 09:59:36 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 09:59:36 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 09:59:36 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   Qwen/Qwen2.5-3B-Instruct-AWQ
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 09:59:36 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 09:59:36 [utils.py:325] 
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 09:59:36 [utils.py:261] non-default args: {'model_tag': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'api_server_count': 1, 'host': '0.0.0.0', 'model': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'max_model_len': 8192, 'quantization': 'awq', 'gpu_memory_utilization': 0.5}
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 09:59:37 [model.py:541] Resolved architecture: Qwen2ForCausalLM
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 09:59:37 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 09:59:37 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 09:59:37 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 09:59:38 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=12459)[0;0m INFO 02-08 09:59:46 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='Qwen/Qwen2.5-3B-Instruct-AWQ', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen2.5-3B-Instruct-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=12459)[0;0m INFO 02-08 09:59:46 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.0.76:45815 backend=nccl
[0;36m(EngineCore_DP0 pid=12459)[0;0m INFO 02-08 09:59:46 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=12459)[0;0m INFO 02-08 09:59:47 [gpu_model_runner.py:4021] Starting to load model Qwen/Qwen2.5-3B-Instruct-AWQ...
[0;36m(EngineCore_DP0 pid=12459)[0;0m INFO 02-08 09:59:47 [cuda.py:364] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=12459)[0;0m INFO 02-08 09:59:48 [weight_utils.py:567] No model.safetensors.index.json found in remote.
[0;36m(EngineCore_DP0 pid=12459)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=12459)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.36it/s]
[0;36m(EngineCore_DP0 pid=12459)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.36it/s]
[0;36m(EngineCore_DP0 pid=12459)[0;0m 
[0;36m(EngineCore_DP0 pid=12459)[0;0m INFO 02-08 09:59:49 [default_loader.py:291] Loading weights took 0.31 seconds
[0;36m(EngineCore_DP0 pid=12459)[0;0m INFO 02-08 09:59:49 [gpu_model_runner.py:4118] Model loading took 1.95 GiB memory and 1.741344 seconds
[0;36m(EngineCore_DP0 pid=12459)[0;0m INFO 02-08 09:59:57 [backends.py:805] Using cache directory: /home/zodrak/.cache/vllm/torch_compile_cache/4d19c4f626/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=12459)[0;0m INFO 02-08 09:59:57 [backends.py:865] Dynamo bytecode transform time: 7.63 s
[0;36m(EngineCore_DP0 pid=12459)[0;0m INFO 02-08 10:00:05 [backends.py:267] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 0.794 s
[0;36m(EngineCore_DP0 pid=12459)[0;0m INFO 02-08 10:00:05 [monitor.py:34] torch.compile takes 8.43 s in total
[0;36m(EngineCore_DP0 pid=12459)[0;0m INFO 02-08 10:00:06 [gpu_worker.py:356] Available KV cache memory: 2.46 GiB
[0;36m(EngineCore_DP0 pid=12459)[0;0m INFO 02-08 10:00:06 [kv_cache_utils.py:1307] GPU KV cache size: 71,680 tokens
[0;36m(EngineCore_DP0 pid=12459)[0;0m INFO 02-08 10:00:06 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 8.75x
[0;36m(EngineCore_DP0 pid=12459)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|â–         | 1/51 [00:00<00:11,  4.26it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 2/51 [00:00<00:11,  4.27it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 3/51 [00:00<00:10,  4.65it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 4/51 [00:00<00:09,  4.84it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|â–‰         | 5/51 [00:01<00:09,  4.94it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:01<00:08,  5.04it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|â–ˆâ–Ž        | 7/51 [00:01<00:08,  5.07it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 8/51 [00:01<00:08,  5.12it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 9/51 [00:01<00:08,  5.17it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–‰        | 10/51 [00:02<00:07,  5.20it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 11/51 [00:02<00:07,  5.20it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:02<00:07,  5.22it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:02<00:06,  5.44it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:02<00:06,  5.64it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:02<00:06,  5.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:03<00:05,  5.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:03<00:05,  6.26it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:03<00:05,  6.59it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:03<00:04,  6.82it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:03<00:04,  7.02it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:03<00:04,  7.18it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:03<00:03,  7.32it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:03<00:03,  7.41it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:04<00:03,  7.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:04<00:03,  7.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:04<00:03,  7.59it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:04<00:03,  7.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:04<00:03,  7.47it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:04<00:02,  7.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:04<00:02,  8.01it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:04<00:02,  8.07it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:05<00:02,  8.07it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:05<00:02,  8.39it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:05<00:01,  8.64it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:05<00:01,  8.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:05<00:01,  8.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [00:05<00:01,  8.77it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:05<00:01,  9.07it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [00:05<00:01,  9.52it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [00:06<00:00, 10.05it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:06<00:00, 10.41it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [00:06<00:00, 10.54it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [00:06<00:00, 10.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [00:06<00:00, 11.00it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:06<00:00,  7.34it/s]
[0;36m(EngineCore_DP0 pid=12459)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   3%|â–Ž         | 1/35 [00:00<00:05,  6.74it/s]Capturing CUDA graphs (decode, FULL):   6%|â–Œ         | 2/35 [00:00<00:04,  7.08it/s]Capturing CUDA graphs (decode, FULL):   9%|â–Š         | 3/35 [00:00<00:04,  7.16it/s]Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:04,  7.19it/s]Capturing CUDA graphs (decode, FULL):  14%|â–ˆâ–        | 5/35 [00:00<00:04,  7.27it/s]Capturing CUDA graphs (decode, FULL):  17%|â–ˆâ–‹        | 6/35 [00:00<00:03,  7.35it/s]Capturing CUDA graphs (decode, FULL):  20%|â–ˆâ–ˆ        | 7/35 [00:00<00:03,  7.35it/s]Capturing CUDA graphs (decode, FULL):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:01<00:03,  7.42it/s]Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:01<00:03,  7.63it/s]Capturing CUDA graphs (decode, FULL):  29%|â–ˆâ–ˆâ–Š       | 10/35 [00:01<00:03,  7.49it/s]Capturing CUDA graphs (decode, FULL):  31%|â–ˆâ–ˆâ–ˆâ–      | 11/35 [00:01<00:03,  7.38it/s]Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:01<00:03,  7.34it/s]Capturing CUDA graphs (decode, FULL):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [00:01<00:02,  7.68it/s]Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:01<00:02,  7.96it/s]Capturing CUDA graphs (decode, FULL):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15/35 [00:01<00:02,  8.17it/s]Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:02<00:02,  8.26it/s]Capturing CUDA graphs (decode, FULL):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [00:02<00:02,  8.63it/s]Capturing CUDA graphs (decode, FULL):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:02<00:01,  8.92it/s]Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:02<00:01,  9.03it/s]Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:02<00:01,  9.18it/s]Capturing CUDA graphs (decode, FULL):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:02<00:01,  9.29it/s]Capturing CUDA graphs (decode, FULL):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 23/35 [00:02<00:01,  9.75it/s]Capturing CUDA graphs (decode, FULL):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25/35 [00:03<00:00, 10.19it/s]Capturing CUDA graphs (decode, FULL):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 27/35 [00:03<00:00, 10.71it/s]Capturing CUDA graphs (decode, FULL):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 29/35 [00:03<00:00, 11.01it/s]Capturing CUDA graphs (decode, FULL):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 31/35 [00:03<00:00, 11.14it/s]Capturing CUDA graphs (decode, FULL):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 33/35 [00:03<00:00, 11.31it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:03<00:00, 11.54it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:03<00:00,  9.08it/s]
[0;36m(EngineCore_DP0 pid=12459)[0;0m INFO 02-08 10:00:17 [gpu_model_runner.py:5051] Graph capturing finished in 11 secs, took 0.72 GiB
[0;36m(EngineCore_DP0 pid=12459)[0;0m INFO 02-08 10:00:17 [core.py:272] init engine (profile, create kv cache, warmup model) took 28.09 seconds
[0;36m(EngineCore_DP0 pid=12459)[0;0m INFO 02-08 10:00:18 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:19 [api_server.py:665] Supported tasks: ['generate']
[0;36m(APIServer pid=12207)[0;0m WARNING 02-08 10:00:19 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:19 [serving.py:177] Warming up chat template processing...
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [hf.py:310] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [serving.py:212] Chat template warmup completed in 1548.4ms
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:00:21 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=12207)[0;0m INFO:     Started server process [12207]
[0;36m(APIServer pid=12207)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=12207)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=12207)[0;0m INFO 02-08 10:06:10 [launcher.py:110] Shutting down FastAPI HTTP server.
[0;36m(APIServer pid=12207)[0;0m INFO:     Shutting down
[0;36m(APIServer pid=12207)[0;0m INFO:     Waiting for application shutdown.
[0;36m(APIServer pid=12207)[0;0m INFO:     Application shutdown complete.
